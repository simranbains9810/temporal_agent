{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simranbains9810/temporal_agent/blob/main/temporal_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycM5y4eTSW9J"
      },
      "source": [
        "#Temporal Agent for Financial Transcripts\n",
        "\n",
        "**Author:** Simran Bains\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "This project develops a **time-aware RAG system** for financial earnings call transcripts, where earlier statements are marked as expired once newer updates appear. It combines semantic chunking, fact extraction with timestamps, entity resolution, temporal invalidation, and knowledge graph construction. The result is a dynamic knowledge base that can answer **time-sensitive questions** about how a company’s outlook changes over time.\n",
        "\n",
        "**Code repository:** https://github.com/simranbains9810/temporal_agent\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "1. [Notebook Setup](#notebook-setup)  \n",
        "2. [Pre-processing and Analyzing our Dynamic Data](#pre-processing-and-analysing-our-dynamic-data)  \n",
        "3. [Percentile Based Chunking](#percentile-based-chunking)  \n",
        "4. [Extracting Atomic Facts with a Statement Agent](#extracting-atomic-facts-with-a-statement-agent)  \n",
        "5. [Pinpointing Time with a Validation Check Agent](#pinpointing-time-with-a-validation-check-agent)  \n",
        "6. [Structuring Facts into Triplets](#structuring-facts-into-triplets)\n",
        "7. [Assembling the Temporal Event](#assembling-the-temporal-event)\n",
        "8. [Automating the Pipeline with LangGraph](#automating-the-pipeline-with-LangGraph)\n",
        "9. [Cleaning Our Data with Entity Resolution](#cleaning-our-data-with-entity-resolution)\n",
        "10. [Making Our Knowledge Dynamic with an Invalidation Agent](#making-our-knowledge-dynamic-with-an-invalidation-agent)\n",
        "11. [Assembling the Temporal Knowledge Graph](#assembling-the-temporal-knowledge-graph)\n",
        "12. [Building and Testing A Multi-Step Retrieval Agent](#building-and-testing-a-multi-step-retrieval-agent)\n",
        "\n",
        "---\n",
        "\n",
        "## Notebook Setup\n",
        "- Core: `langchain-community`, `datasets==2.19.0`, `langchain-experimental`, `langgraph`\n",
        "- Embeddings/LLM: `langchain-nebius`, `openai` (indirect), `tiktoken`\n",
        "- Data/Utils: `pandas`, `numpy`, `pyarrow`, `python-dateutil`, `rapidfuzz`, `networkx`, `matplotlib`, `scipy`\n",
        "- (Colab will already include many of these.)\n",
        "\n",
        "### Install/Upgrade Dependencies\n",
        "```bash\n",
        "# Base installs (pin datasets to 2.19.0; langchain-* at current project versions)\n",
        "pip install -q \"datasets==2.19.0\" langchain-community langchain-experimental langgraph \\\n",
        "               langchain-nebius rapidfuzz networkx matplotlib scipy\n",
        "\n",
        "# Optional: if you hit storage/FS conflicts, align fsspec with datasets 2.19.0 constraints\n",
        "pip install -q \"fsspec==2024.3.1\" || true\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-processing and Analyzing our Dynamic Data"
      ],
      "metadata": {
        "id": "LftJVNMl9WId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the pre-processing stage we use HuggingFace to extract the ilh-ibm/earnings call dataset, which gives us 188 transcripts from about ten big tech companies. Each transcript comes with a company name and date, so we already have a natural time anchor. The transcripts are pretty hefty - around 9,000 words on average. The first step is to check how the data is distributed across companies by running some basic stats - I had pulled out quarter references like \"Q2 2016\" from the text so it could be identified which time period each transcript was describing."
      ],
      "metadata": {
        "id": "qRb9qkc8Bzeg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFIeWIsLTKW8",
        "outputId": "6a6f5c8a-e32f-4382-c0ec-34e6a2c1b570"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting datasets==2.19.0\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.19.0)\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (0.70.16)\n",
            "Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.0)\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (3.12.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (6.0.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.14)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.2->datasets==2.19.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.2->datasets==2.19.0) (1.1.7)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.19.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.19.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.19.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.19.0) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.0) (2025.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.0) (1.17.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, pyarrow-hotfix, mypy-extensions, marshmallow, httpx-sse, fsspec, typing-inspect, pydantic-settings, dataclasses-json, datasets, langchain-community\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nccl-cu12==2.21.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nccl-cu12 2.23.4 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 datasets-2.19.0 fsspec-2024.3.1 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pyarrow-hotfix-0.7 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install langchain-community \"datasets==2.19.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJKj_fhTRT8h",
        "outputId": "6ff2531f-cab0-49ed-e506-50c69a2b9162"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/datasets/load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import loader for Hugging Face datasets\n",
        "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n",
        "\n",
        "# Dataset configuration\n",
        "hf_dataset_name = \"jlh-ibm/earnings_call\"  # HF dataset name\n",
        "subset_name = \"transcripts\"                # Dataset subset to load\n",
        "\n",
        "# Create the loader (defaults to 'train' split)\n",
        "loader = HuggingFaceDatasetLoader(\n",
        "    path=hf_dataset_name,\n",
        "    name=subset_name,\n",
        "    page_content_column=\"transcript\"  # Column containing the main text\n",
        ")\n",
        "\n",
        "# This is the key step. The loader processes the dataset and returns a list of LangChain Document objects.\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeZmKSNKTcSh",
        "outputId": "58231ba0-d6a6-434c-9413-b2f7a936057e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 188 documents.\n"
          ]
        }
      ],
      "source": [
        "# Let's inspect the result to see the difference\n",
        "print(f\"Loaded {len(documents)} documents.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CH47ozUVYnqw",
        "outputId": "1c545b7f-f085-4738-8cf7-a72371e1d35a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total company counts:\n",
            " - AMD: 19\n",
            " - AAPL: 19\n",
            " - INTC: 19\n",
            " - MU: 17\n",
            " - GOOGL: 19\n",
            " - ASML: 19\n",
            " - CSCO: 19\n",
            " - NVDA: 19\n",
            " - AMZN: 19\n",
            " - MSFT: 19\n"
          ]
        }
      ],
      "source": [
        "# Count how many documents each company has\n",
        "company_counts = {}\n",
        "\n",
        "# Loop over all loaded documents\n",
        "for doc in documents:\n",
        "    company = doc.metadata.get(\"company\")  # Extract company from metadata\n",
        "    if company:\n",
        "        company_counts[company] = company_counts.get(company, 0) + 1\n",
        "\n",
        "# Display the counts\n",
        "print(\"Total company counts:\")\n",
        "for company, count in company_counts.items():\n",
        "    print(f\" - {company}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlLdXLKwY6wX",
        "outputId": "0be0df80-9b5a-4c7c-e9bf-9853e32253ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metadata for document[0]:\n",
            "{'company': 'AMD', 'date': datetime.date(2016, 7, 21)}\n",
            "\n",
            "Metadata for document[33]:\n",
            "{'company': 'AMZN', 'date': datetime.date(2019, 10, 24)}\n"
          ]
        }
      ],
      "source": [
        "# Print metadata for two sample documents (index 0 and 33)\n",
        "print(\"Metadata for document[0]:\")\n",
        "print(documents[0].metadata)\n",
        "\n",
        "print(\"\\nMetadata for document[33]:\")\n",
        "print(documents[33].metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXO6OHuyZBw9",
        "outputId": "712c6f8c-83c0-4c23-d0ef-5bf87c778a25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"\\n\\nThomson Reuters StreetEvents Event Transcript\\nE D I T E D   V E R S I O N\\n\\nQ2 2016 Advanced Micro Devices Inc Earnings Call\\nJULY 21, 2016 / 9:00PM GMT\\n\\n=====================================\n"
          ]
        }
      ],
      "source": [
        "# Print the first 200 characters of the first document's content\n",
        "first_doc = documents[0]\n",
        "print(first_doc.page_content[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0lwBkSSZCDc",
        "outputId": "8e88b3aa-836e-4f02-9ff5-04e286014259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average number of words in documents: 8797.19\n"
          ]
        }
      ],
      "source": [
        "# Calculate the average number of words per document\n",
        "total_words = sum(len(doc.page_content.split()) for doc in documents)\n",
        "average_words = total_words / len(documents) if documents else 0\n",
        "\n",
        "print(f\"Average number of words in documents: {average_words:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW_mOy_wZEAT",
        "outputId": "96433a1a-397d-406a-ef17-7c4f2a8d3b7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted Quarter for the first document: Q2 2016\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# Helper function to extract a quarter string (e.g., \"Q1 2023\") from text\n",
        "def find_quarter(text: str) -> str | None:\n",
        "    \"\"\"Return the first quarter-year match found in the text, or None if absent.\"\"\"\n",
        "    # Match pattern: 'Q' followed by 1 digit, a space, and a 4-digit year\n",
        "    match = re.findall(r\"Q\\d\\s\\d{4}\", text)\n",
        "    return match[0] if match else None\n",
        "\n",
        "# Test on the first document\n",
        "quarter = find_quarter(documents[0].page_content)\n",
        "print(f\"Extracted Quarter for the first document: {quarter}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRU6ENhmk1E2"
      },
      "source": [
        "##Percentile Based Chunking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRwBQ2rylN3E"
      },
      "source": [
        "This section explains how percentile-based semantic chunking is used to split long financial transcripts into smaller, meaningful segments without losing context. Instead of cutting at fixed lengths or punctuation, each sentence is embedded with a model (Qwen3–8B via Nebius), and semantic distances between consecutive sentences are calculated. The process works as follows:\n",
        "Split text into sentences via regex.\n",
        "\n",
        "*   Split text into sentences via regex.\n",
        "*   Embed each sentence into a vector.\n",
        "*   Compute semantic distance between consecutive vectors.\n",
        "*   Determine the chosen percentile (e.g., 95th) of distances.\n",
        "*   Mark distances ≥ threshold as breakpoints.\n",
        "*   Group sentences between breakpoints into chunks, enforcing minimum size and optional overlap.\n",
        "\n",
        "Using this method, 188 long documents (≈8K words each) are transformed into 3,556 smaller chunks (≈445 words each) while retaining metadata such as company, date, and quarter for later retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nahxxALpm9Yk",
        "outputId": "9f063e23-e1bf-46b0-a02e-ef182ddaae94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-nebius in /usr/local/lib/python3.11/dist-packages (0.1.3)\n",
            "Requirement already satisfied: langchain-core>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain-nebius) (0.3.74)\n",
            "Requirement already satisfied: langchain_openai>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from langchain-nebius) (0.3.30)\n",
            "Requirement already satisfied: openai>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langchain-nebius) (1.99.9)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.35->langchain-nebius) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.35->langchain-nebius) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.35->langchain-nebius) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.35->langchain-nebius) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.35->langchain-nebius) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.35->langchain-nebius) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.35->langchain-nebius) (2.11.7)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai>=0.3.5->langchain-nebius) (0.11.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->langchain-nebius) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->langchain-nebius) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->langchain-nebius) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->langchain-nebius) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->langchain-nebius) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->langchain-nebius) (4.67.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai>=1.0.0->langchain-nebius) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0->langchain-nebius) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0->langchain-nebius) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0->langchain-nebius) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.35->langchain-nebius) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.35->langchain-nebius) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.35->langchain-nebius) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.35->langchain-nebius) (2.32.3)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.35->langchain-nebius) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core>=0.3.35->langchain-nebius) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core>=0.3.35->langchain-nebius) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core>=0.3.35->langchain-nebius) (0.4.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai>=0.3.5->langchain-nebius) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.3.35->langchain-nebius) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.3.35->langchain-nebius) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain-nebius"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahLoXgS9pCvf"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKY-bRGTZIE8"
      },
      "outputs": [],
      "source": [
        "from langchain_nebius import NebiusEmbeddings\n",
        "\n",
        "# Set Nebius API key (⚠️ Avoid hardcoding secrets in production code)\n",
        "os.environ[\"NEBIUS_API_KEY\"] = \"INSERT NEBIUS KEY\"\n",
        "\n",
        "# 1. Initialize Nebius embedding model\n",
        "embeddings = NebiusEmbeddings(model=\"Qwen/Qwen3-Embedding-8B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCXdq2p3pc9n",
        "outputId": "53df6651-b801-4b8d-d420-41433607cd93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-experimental in /usr/local/lib/python3.11/dist-packages (0.3.4)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-experimental) (0.3.27)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.28 in /usr/local/lib/python3.11/dist-packages (from langchain-experimental) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.14)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (2.11.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.9)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain-experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aTvLYXSkzq4"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "# Create a semantic chunker using percentile thresholding\n",
        "langchain_semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\",  # Use percentile-based splitting\n",
        "    breakpoint_threshold_amount=95           # split at 95th percentile\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUlz47C2rtfV"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkxzfsUZpIEo",
        "outputId": "ecd3bb8b-7b02-4ca0-aa9c-f3385c9ba508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 188 documents using LangChain's SemanticChunker...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chunking Transcripts with LangChain: 100%|██████████| 188/188 [10:09:04<00:00, 194.39s/it]\n"
          ]
        }
      ],
      "source": [
        "# Store the new, smaller chunk documents\n",
        "chunked_documents_lc = []\n",
        "\n",
        "# Printing total number of docs (188) We already know that\n",
        "print(f\"Processing {len(documents)} documents using LangChain's SemanticChunker...\")\n",
        "\n",
        "# Chunk each transcript document\n",
        "for doc in tqdm(documents, desc=\"Chunking Transcripts with LangChain\"):\n",
        "    # Extract quarter info and copy existing metadata\n",
        "    quarter = find_quarter(doc.page_content)\n",
        "    parent_metadata = doc.metadata.copy()\n",
        "    parent_metadata[\"quarter\"] = quarter\n",
        "\n",
        "    # Perform semantic chunking (returns Document objects with metadata attached)\n",
        "    chunks = langchain_semantic_chunker.create_documents(\n",
        "        [doc.page_content],\n",
        "        metadatas=[parent_metadata]\n",
        "    )\n",
        "\n",
        "    # Collect all chunks\n",
        "    chunked_documents_lc.extend(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCSKx3Yirkp6",
        "outputId": "d242cacc-1ae8-4570-f9d5-199aa7ed8e46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original number of documents (transcripts): 188\n",
            "Number of new documents (chunks): 4275\n",
            "Average chunks per transcript: 22.74\n"
          ]
        }
      ],
      "source": [
        "# Analyze the results of the LangChain chunking process\n",
        "original_doc_count = len(documents)\n",
        "chunked_doc_count = len(chunked_documents_lc)\n",
        "\n",
        "print(f\"Original number of documents (transcripts): {original_doc_count}\")\n",
        "print(f\"Number of new documents (chunks): {chunked_doc_count}\")\n",
        "print(f\"Average chunks per transcript: {chunked_doc_count / original_doc_count:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GRXTwGfOzPa",
        "outputId": "6070ae92-cd83-47e8-c257-1855e1a9268d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Chunk Content (first 30 chars):\n",
            "No, that's a fair question, Ma...\n",
            "\n",
            "Sample Chunk Metadata:\n",
            "{'company': 'AMD', 'date': datetime.date(2016, 7, 21), 'quarter': 'Q2 2016'}\n",
            "\n",
            "Average number of words per chunk: 386.87\n"
          ]
        }
      ],
      "source": [
        "# Inspect the 11th chunk (index 10)\n",
        "sample_chunk = chunked_documents_lc[10]\n",
        "print(\"Sample Chunk Content (first 30 chars):\")\n",
        "print(sample_chunk.page_content[:30] + \"...\")\n",
        "\n",
        "print(\"\\nSample Chunk Metadata:\")\n",
        "print(sample_chunk.metadata)\n",
        "\n",
        "# Calculate average word count per chunk\n",
        "total_chunk_words = sum(len(doc.page_content.split()) for doc in chunked_documents_lc)\n",
        "average_chunk_words = total_chunk_words / chunked_doc_count if chunked_documents_lc else 0\n",
        "\n",
        "print(f\"\\nAverage number of words per chunk: {average_chunk_words:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extracting Atomic Facts with a Statement Agent"
      ],
      "metadata": {
        "id": "_VF9GAQczyUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have obtained the sentences in sematic chunks, we distil them into atomic statements which are self-contained claims that can stand on their own. Each statement is labelled in two ways (1) by type of claim - fact, opinion or prediction and (2) temporal nature - static (anchored to a point in time), dynamic (can change over time) or atemporal (always true). To make the model outputs consistently structures, we define a schema with pydantic models which essentially acts like a contract: the LLM must return JSON object with each statement plus its labels. By tagging these statements, the Al agent can separate what actually happened from what management thinks or expects to happen."
      ],
      "metadata": {
        "id": "SoE1hDoQ0GNu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1ElO7uAq-z7"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "# Enum for temporal labels describing time sensitivity\n",
        "class TemporalType(str, Enum):\n",
        "    ATEMPORAL = \"ATEMPORAL\"  # Facts that are always true (e.g., \"Earth is a planet\")\n",
        "    STATIC = \"STATIC\"        # Facts about a single point in time (e.g., \"Product X launched on Jan 1st\")\n",
        "    DYNAMIC = \"DYNAMIC\"      # Facts describing an ongoing state (e.g., \"Lisa Su is the CEO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pf9YVr5PsC6R"
      },
      "outputs": [],
      "source": [
        "# Enum for statement labels classifying statement nature\n",
        "class StatementType(str, Enum):\n",
        "    FACT = \"FACT\"            # An objective, verifiable claim\n",
        "    OPINION = \"OPINION\"      # A subjective belief or judgment\n",
        "    PREDICTION = \"PREDICTION\"  # A statement about a future event"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvF_3BXMsGqC"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, field_validator\n",
        "\n",
        "# This model defines the structure for a single extracted statement\n",
        "class RawStatement(BaseModel):\n",
        "    statement: str\n",
        "    statement_type: StatementType\n",
        "    temporal_type: TemporalType\n",
        "\n",
        "# This model is a container for the list of statements from one chunk\n",
        "class RawStatementList(BaseModel):\n",
        "    statements: list[RawStatement]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPjIKE5GsIn1"
      },
      "outputs": [],
      "source": [
        "# These definitions provide the necessary context for the LLM to understand the labels.\n",
        "LABEL_DEFINITIONS: dict[str, dict[str, dict[str, str]]] = {\n",
        "    \"episode_labelling\": {\n",
        "        \"FACT\": dict(definition=\"Statements that are objective and can be independently verified or falsified through evidence.\"),\n",
        "        \"OPINION\": dict(definition=\"Statements that contain personal opinions, feelings, values, or judgments that are not independently verifiable.\"),\n",
        "        \"PREDICTION\": dict(definition=\"Uncertain statements about the future on something that might happen, a hypothetical outcome, unverified claims.\"),\n",
        "    },\n",
        "    \"temporal_labelling\": {\n",
        "        \"STATIC\": dict(definition=\"Often past tense, think -ed verbs, describing single points-in-time.\"),\n",
        "        \"DYNAMIC\": dict(definition=\"Often present tense, think -ing verbs, describing a period of time.\"),\n",
        "        \"ATEMPORAL\": dict(definition=\"Statements that will always hold true regardless of time.\"),\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrq9ce0psKwD"
      },
      "outputs": [],
      "source": [
        "# Format label definitions into a clean string for prompt injection\n",
        "definitions_text = \"\"\n",
        "\n",
        "for section_key, section_dict in LABEL_DEFINITIONS.items():\n",
        "    # Add a section header with underscores replaced by spaces and uppercased\n",
        "    definitions_text += f\"==== {section_key.replace('_', ' ').upper()} DEFINITIONS ====\\n\"\n",
        "\n",
        "    # Add each category and its definition under the section\n",
        "    for category, details in section_dict.items():\n",
        "        definitions_text += f\"- {category}: {details.get('definition', '')}\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izmoQNnjsNjs"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define the prompt template for statement extraction and labeling\n",
        "statement_extraction_prompt_template = \"\"\"\n",
        "You are an expert extracting atomic statements from text.\n",
        "\n",
        "Inputs:\n",
        "- main_entity: {main_entity}\n",
        "- document_chunk: {document_chunk}\n",
        "\n",
        "Tasks:\n",
        "1. Extract clear, single-subject statements.\n",
        "2. Label each as FACT, OPINION, or PREDICTION.\n",
        "3. Label each temporally as STATIC, DYNAMIC, or ATEMPORAL.\n",
        "4. Resolve references to main_entity and include dates/quantities.\n",
        "\n",
        "Return ONLY a JSON object with the statements and labels.\n",
        "\"\"\"\n",
        "\n",
        "# Create a ChatPromptTemplate from the string template\n",
        "prompt = ChatPromptTemplate.from_template(statement_extraction_prompt_template)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CWSPRlSsP38"
      },
      "outputs": [],
      "source": [
        "from langchain_nebius import ChatNebius\n",
        "import json\n",
        "\n",
        "# Initialize our LLM\n",
        "llm = ChatNebius(model=\"deepseek-ai/DeepSeek-V3\")\n",
        "\n",
        "# Create the chain: prompt -> LLM -> structured output parser\n",
        "statement_extraction_chain = prompt | llm.with_structured_output(RawStatementList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fvlKvNTsSp_",
        "outputId": "1d1e9b9f-8156-4823-e3ed-648f071cdee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running statement extraction on a sample chunk ---\n",
            "Chunk Content:\n",
            "No, that's a fair question, Matt. So we have been very focused on the server launch for first half of 2017. Desktop should launch before that. In terms of true volume availability, I believe it will be in the first quarter of 2017. We may ship some limited volume towards the end of the fourth quarter, based on how bring-up goes and the customer readiness.\\nBut again, if I look overall at what we are trying to do, I think the desktop product is very well-positioned for that high-end desktop segment, that enthusiast segment, in both channel and OEM, which is very much a segment that AMD knows well. And so that's where we would focus -- on desktop.\\nYou should expect a notebook version of Zen with integrated graphics in 2017, and that development is going on as well. And so I think it's just a time of a lot of activity around the Zen and the different Zen product families.\\n\\n--------------------------------------------------------------------------------\\nMatt Ramsay,  Canaccord Genuity - Analyst    [22]\\n--------------------------------------------------------------------------------\\nThank you very much. Congratulations on the return to profitability.\\n\\n--------------------------------------------------------------------------------\\nLisa Su,  Advanced Micro Devices, Inc. - President and CEO    [23]\\n--------------------------------------------------------------------------------\\nThanks, Matt.\\n\\n--------------------------------------------------------------------------------\\nDevinder Kumar,  Advanced Micro Devices, Inc. - SVP, CFO and Treasurer    [24]\\n--------------------------------------------------------------------------------\\nThank you.\\n\\n--------------------------------------------------------------------------------\\nOperator    [25]\\n--------------------------------------------------------------------------------\\nStacy Rasgon, Bernstein Research.\\n\\n--------------------------------------------------------------------------------\\nStacy Rasgon,  Bernstein Research - Analyst    [26]\\n--------------------------------------------------------------------------------\\nThanks for taking my questions. I was looking at the implied guidance for Q4. You said the -- I guess for the full year, up low single digits. So, I mean, call it 3%. But it would imply Q4 down 16%/17% sequentially, and actually down on an absolute basis, lower than I would've thought. I think Q4 also has an extra week in it.\\nI was wondering if you could give us, I guess, some color on how you see the drivers, I guess, for seasonality going from Q3 to Q4 across both of the businesses, given that, I guess, the trajectory of the different product launches that we have in the back half? Like, how do you come to that number?\\n\\n--------------------------------------------------------------------------------\\nLisa Su,  Advanced Micro Devices, Inc. - President and CEO    [27]\\n--------------------------------------------------------------------------------\\nSure. Stacy, maybe I'll start and see if Devinder would like to add to it. So, look, our -- when we started the year, our expectation is that we would grow revenue in 2016 versus 2015, but we were coming off of a very low base in the first quarter. So we've been pleased with how it played out certainly in our second-quarter revenue and the third quarter revenue guidance.\\nOverall, the businesses are performing well, so we do expect both Computing and Graphics and EEFC to both grow for the year. I think the semicustom business is the large driver of the fourth quarter in terms of just how we see the overall business playing out. But the Computing and Graphics business is playing out as you might expect.\\nSo the second half should be seasonally higher, certainly with Polaris, and as we launch broader availability across the product line, as well as the seventh generation APUs as they go into back-to-school and holiday. So that's the way we should think about it.\\n\\n--------------------------------------------------------------------------------\\nStacy Rasgon,  Bernstein Research - Analyst    [28]\\n--------------------------------------------------------------------------------\\nOkay.\n",
            "\n",
            "Invoking LLM for extraction...\n",
            "\n",
            "--- Extraction Result ---\n",
            "{\n",
            "  \"statements\": [\n",
            "    {\n",
            "      \"statement\": \"AMD has been very focused on the server launch for the first half of 2017.\",\n",
            "      \"statement_type\": \"FACT\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD's desktop product should launch before the server launch in the first half of 2017.\",\n",
            "      \"statement_type\": \"PREDICTION\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD expects true volume availability for the desktop product in the first quarter of 2017.\",\n",
            "      \"statement_type\": \"PREDICTION\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD may ship some limited desktop product volume towards the end of the fourth quarter of 2016.\",\n",
            "      \"statement_type\": \"PREDICTION\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD believes the desktop product is very well-positioned for the high-end desktop segment and enthusiast segment.\",\n",
            "      \"statement_type\": \"OPINION\",\n",
            "      \"temporal_type\": \"STATIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD knows the high-end desktop and enthusiast segment well.\",\n",
            "      \"statement_type\": \"FACT\",\n",
            "      \"temporal_type\": \"STATIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD will focus on the desktop product.\",\n",
            "      \"statement_type\": \"FACT\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD expects a notebook version of Zen with integrated graphics in 2017.\",\n",
            "      \"statement_type\": \"PREDICTION\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD is developing a notebook version of Zen with integrated graphics.\",\n",
            "      \"statement_type\": \"FACT\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD is experiencing a lot of activity around the Zen and different Zen product families.\",\n",
            "      \"statement_type\": \"FACT\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD returned to profitability.\",\n",
            "      \"statement_type\": \"FACT\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD expects revenue in 2016 to grow versus 2015.\",\n",
            "      \"statement_type\": \"PREDICTION\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD's semicustom business is the large driver of the fourth quarter of 2016.\",\n",
            "      \"statement_type\": \"FACT\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD's Computing and Graphics business is performing as expected.\",\n",
            "      \"statement_type\": \"FACT\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD expects the second half of 2016 to be seasonally higher due to Polaris and broader product availability.\",\n",
            "      \"statement_type\": \"PREDICTION\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD's seventh generation APUs are expected to go into back-to-school and holiday seasons in 2016.\",\n",
            "      \"statement_type\": \"PREDICTION\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Select the sample chunk we inspected earlier for testing extraction\n",
        "sample_chunk_for_extraction = chunked_documents_lc[10]\n",
        "\n",
        "print(\"--- Running statement extraction on a sample chunk ---\")\n",
        "print(f\"Chunk Content:\\n{sample_chunk_for_extraction.page_content}\")\n",
        "print(\"\\nInvoking LLM for extraction...\")\n",
        "\n",
        "# Call the extraction chain with necessary inputs\n",
        "extracted_statements_list = statement_extraction_chain.invoke({\n",
        "    \"main_entity\": sample_chunk_for_extraction.metadata[\"company\"],\n",
        "    \"publication_date\": sample_chunk_for_extraction.metadata[\"date\"].isoformat(),\n",
        "    \"document_chunk\": sample_chunk_for_extraction.page_content,\n",
        "    \"definitions\": definitions_text\n",
        "})\n",
        "\n",
        "print(\"\\n--- Extraction Result ---\")\n",
        "# Pretty-print the output JSON from the model response\n",
        "print(extracted_statements_list.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pinpointing Time with a Validation Check Agent"
      ],
      "metadata": {
        "id": "0yQz45fH0Yvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next stage is to attach precise timestamps to each fact so that our knowledge base becomes temporal. I.e. you might have a statement \"AMD has been focused on server launch for the first half of 2017\" which doesn't have a proper time anchor. To solve this we build a specialised date agent which (1) uses the publication date of the document as a reference point (2) interprets fuzzy phrases like \"next quarter\" or \"three months ago\" and (3) converts them to exact time ranges 2017-01-01 'valid at' and 2017-06-31 'invalid at' (when it became true) which is when the statement stopped being rue (if it has)."
      ],
      "metadata": {
        "id": "EfLwxgKG0fQ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTgub_mGskk8"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timezone\n",
        "from dateutil.parser import parse\n",
        "import re\n",
        "\n",
        "def parse_date_str(value: str | datetime | None) -> datetime | None:\n",
        "    \"\"\"\n",
        "    Parse a string or datetime into a timezone-aware datetime object (UTC).\n",
        "    Returns None if parsing fails or input is None.\n",
        "    \"\"\"\n",
        "    if not value:\n",
        "        return None\n",
        "\n",
        "    # If already a datetime, ensure it has timezone info (UTC if missing)\n",
        "    if isinstance(value, datetime):\n",
        "        return value if value.tzinfo else value.replace(tzinfo=timezone.utc)\n",
        "\n",
        "    try:\n",
        "        # Handle year-only strings like \"2023\"\n",
        "        if re.fullmatch(r\"\\d{4}\", value.strip()):\n",
        "            year = int(value.strip())\n",
        "            return datetime(year, 1, 1, tzinfo=timezone.utc)\n",
        "\n",
        "        # Parse more complex date strings with dateutil\n",
        "        dt: datetime = parse(value)\n",
        "\n",
        "        # Ensure timezone-aware, default to UTC if missing\n",
        "        if dt.tzinfo is None:\n",
        "            dt = dt.replace(tzinfo=timezone.utc)\n",
        "\n",
        "        return dt\n",
        "    except Exception:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YDXNUV7slHD"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field, field_validator\n",
        "from datetime import datetime\n",
        "\n",
        "# Model for raw temporal range with date strings as ISO 8601\n",
        "class RawTemporalRange(BaseModel):\n",
        "    valid_at: str | None = Field(None, description=\"The start date/time in ISO 8601 format.\")\n",
        "    invalid_at: str | None = Field(None, description=\"The end date/time in ISO 8601 format.\")\n",
        "\n",
        "# Model for validated temporal range with datetime objects\n",
        "class TemporalValidityRange(BaseModel):\n",
        "    valid_at: datetime | None = None\n",
        "    invalid_at: datetime | None = None\n",
        "\n",
        "    # Validator to parse date strings into datetime objects before assignment\n",
        "    @field_validator(\"valid_at\", \"invalid_at\", mode=\"before\")\n",
        "    @classmethod\n",
        "    def _parse_date_string(cls, value: str | datetime | None) -> datetime | None:\n",
        "        return parse_date_str(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUmYsIARsnMH"
      },
      "outputs": [],
      "source": [
        "# Prompt guiding the LLM to extract temporal validity ranges from statements\n",
        "date_extraction_prompt_template = \"\"\"\n",
        "You are a temporal information extraction specialist.\n",
        "\n",
        "INPUTS:\n",
        "- statement: \"{statement}\"\n",
        "- statement_type: \"{statement_type}\"\n",
        "- temporal_type: \"{temporal_type}\"\n",
        "- publication_date: \"{publication_date}\"\n",
        "- quarter: \"{quarter}\"\n",
        "\n",
        "TASK:\n",
        "- Analyze the statement and determine the temporal validity range (valid_at, invalid_at).\n",
        "- Use the publication date as the reference point for relative expressions (e.g., \"currently\").\n",
        "- If a relationship is ongoing or its end is not specified, `invalid_at` should be null.\n",
        "\n",
        "GUIDANCE:\n",
        "- For STATIC statements, `valid_at` is the date the event occurred, and `invalid_at` is null.\n",
        "- For DYNAMIC statements, `valid_at` is when the state began, and `invalid_at` is when it ended.\n",
        "- Return dates in ISO 8601 format (e.g., YYYY-MM-DDTHH:MM:SSZ).\n",
        "\n",
        "**Output format**\n",
        "Return ONLY a valid JSON object matching the schema for `RawTemporalRange`.\n",
        "\"\"\"\n",
        "\n",
        "# Create a LangChain prompt template from the string\n",
        "date_extraction_prompt = ChatPromptTemplate.from_template(date_extraction_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgOFCOXisrtp"
      },
      "outputs": [],
      "source": [
        "# Reuse the existing LLM instance.\n",
        "# Create a chain by connecting the date extraction prompt\n",
        "# with the LLM configured to output structured RawTemporalRange objects.\n",
        "date_extraction_chain = date_extraction_prompt | llm.with_structured_output(RawTemporalRange)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meydLprJsu6A",
        "outputId": "b8625240-928d-4c69-cfa0-2dfcad5bf28f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running date extraction for statement ---\n",
            "Statement: \"AMD has been very focused on the server launch for the first half of 2017.\"\n",
            "Reference Publication Date: 2016-07-21\n",
            "\n",
            "--- Parsed & Validated Result ---\n",
            "Valid At: 2017-01-01 00:00:00+00:00\n",
            "Invalid At: 2017-06-30 23:59:59+00:00\n"
          ]
        }
      ],
      "source": [
        "# Take the first extracted statement for date extraction testing\n",
        "sample_statement = extracted_statements_list.statements[0]\n",
        "chunk_metadata = sample_chunk_for_extraction.metadata\n",
        "\n",
        "print(f\"--- Running date extraction for statement ---\")\n",
        "print(f'Statement: \"{sample_statement.statement}\"')\n",
        "print(f\"Reference Publication Date: {chunk_metadata['date'].isoformat()}\")\n",
        "\n",
        "# Invoke the date extraction chain with relevant inputs\n",
        "raw_temporal_range = date_extraction_chain.invoke({\n",
        "    \"statement\": sample_statement.statement,\n",
        "    \"statement_type\": sample_statement.statement_type.value,\n",
        "    \"temporal_type\": sample_statement.temporal_type.value,\n",
        "    \"publication_date\": chunk_metadata[\"date\"].isoformat(),\n",
        "    \"quarter\": chunk_metadata[\"quarter\"]\n",
        "})\n",
        "\n",
        "# Parse and validate raw LLM output into a structured TemporalValidityRange model\n",
        "final_temporal_range = TemporalValidityRange.model_validate(raw_temporal_range.model_dump())\n",
        "\n",
        "print(\"\\n--- Parsed & Validated Result ---\")\n",
        "print(f\"Valid At: {final_temporal_range.valid_at}\")\n",
        "print(f\"Invalid At: {final_temporal_range.invalid_at}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structuring Facts into Triplets"
      ],
      "metadata": {
        "id": "4cc2FiZs1QgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this stage we know the facts (the what) and the dates (the when) we've already extracted and break them into a triplet structure. (1) the subject - the main entity (AMD) (2) the predicate - the relationship or action (3) the object - the related entity or concept. This format is later helpful to connect them into a knowledge graph."
      ],
      "metadata": {
        "id": "aVrM1vi51Tp7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcq9Fa8Nsxn7"
      },
      "outputs": [],
      "source": [
        "from enum import Enum  # Import the Enum base class to create enumerated constants\n",
        "\n",
        "# Enum representing a fixed set of relationship predicates for graph consistency\n",
        "class Predicate(str, Enum):\n",
        "    # Each member of this Enum represents a specific type of relationship between entities\n",
        "    IS_A = \"IS_A\"                # Represents an \"is a\" relationship (e.g., a Dog IS_A Animal)\n",
        "    HAS_A = \"HAS_A\"              # Represents possession or composition (e.g., a Car HAS_A Engine)\n",
        "    LOCATED_IN = \"LOCATED_IN\"    # Represents location relationship (e.g., Store LOCATED_IN City)\n",
        "    HOLDS_ROLE = \"HOLDS_ROLE\"    # Represents role or position held (e.g., Person HOLDS_ROLE Manager)\n",
        "    PRODUCES = \"PRODUCES\"        # Represents production or creation relationship\n",
        "    SELLS = \"SELLS\"              # Represents selling relationship between entities\n",
        "    LAUNCHED = \"LAUNCHED\"        # Represents launch events (e.g., Product LAUNCHED by Company)\n",
        "    DEVELOPED = \"DEVELOPED\"      # Represents development relationship (e.g., Software DEVELOPED by Team)\n",
        "    ADOPTED_BY = \"ADOPTED_BY\"    # Represents adoption relationship (e.g., Policy ADOPTED_BY Organization)\n",
        "    INVESTS_IN = \"INVESTS_IN\"    # Represents investment relationships (e.g., Company INVESTS_IN Startup)\n",
        "    COLLABORATES_WITH = \"COLLABORATES_WITH\"  # Represents collaboration between entities\n",
        "    SUPPLIES = \"SUPPLIES\"        # Represents supplier relationship (e.g., Supplier SUPPLIES Parts)\n",
        "    HAS_REVENUE = \"HAS_REVENUE\"  # Represents revenue relationship for entities\n",
        "    INCREASED = \"INCREASED\"      # Represents an increase event or metric change\n",
        "    DECREASED = \"DECREASED\"      # Represents a decrease event or metric change\n",
        "    RESULTED_IN = \"RESULTED_IN\"  # Represents causal relationship (e.g., Event RESULTED_IN Outcome)\n",
        "    TARGETS = \"TARGETS\"          # Represents target or goal relationship\n",
        "    PART_OF = \"PART_OF\"          # Represents part-whole relationship (e.g., Wheel PART_OF Car)\n",
        "    DISCONTINUED = \"DISCONTINUED\" # Represents discontinued status or event\n",
        "    SECURED = \"SECURED\"          # Represents secured or obtained relationship (e.g., Funding SECURED by Company)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umy_ataXs2oC"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "\n",
        "# Model representing an entity extracted by the LLM\n",
        "class RawEntity(BaseModel):\n",
        "    entity_idx: int = Field(description=\"A temporary, 0-indexed ID for this entity.\")\n",
        "    name: str = Field(description=\"The name of the entity, e.g., 'AMD' or 'Lisa Su'.\")\n",
        "    type: str = Field(\"Unknown\", description=\"The type of entity, e.g., 'Organization', 'Person'.\")\n",
        "    description: str = Field(\"\", description=\"A brief description of the entity.\")\n",
        "\n",
        "# Model representing a single subject-predicate-object triplet\n",
        "class RawTriplet(BaseModel):\n",
        "    subject_name: str\n",
        "    subject_id: int = Field(description=\"The entity_idx of the subject.\")\n",
        "    predicate: Predicate\n",
        "    object_name: str\n",
        "    object_id: int = Field(description=\"The entity_idx of the object.\")\n",
        "    value: Optional[str] = Field(None, description=\"An optional value, e.g., '10%'.\")\n",
        "\n",
        "# Container for all entities and triplets extracted from a single statement\n",
        "class RawExtraction(BaseModel):\n",
        "    entities: List[RawEntity]\n",
        "    triplets: List[RawTriplet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BilAFMTas55B"
      },
      "outputs": [],
      "source": [
        "# These definitions guide the LLM in choosing the correct predicate.\n",
        "PREDICATE_DEFINITIONS = {\n",
        "    \"IS_A\": \"Denotes a class-or-type relationship (e.g., 'Model Y IS_A electric-SUV').\",\n",
        "    \"HAS_A\": \"Denotes a part-whole relationship (e.g., 'Model Y HAS_A electric-engine').\",\n",
        "    \"LOCATED_IN\": \"Specifies geographic or organisational containment.\",\n",
        "    \"HOLDS_ROLE\": \"Connects a person to a formal office or title.\",\n",
        "}\n",
        "\n",
        "# Format the predicate instructions into a string for the prompt.\n",
        "predicate_instructions_text = \"\\n\".join(f\"- {pred}: {desc}\" for pred, desc in PREDICATE_DEFINITIONS.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YSsX7uws8cx"
      },
      "outputs": [],
      "source": [
        "# Prompt for extracting entities and subject-predicate-object triplets from a statement\n",
        "triplet_extraction_prompt_template = \"\"\"\n",
        "You are an information-extraction assistant.\n",
        "\n",
        "Task: From the statement, identify all entities (people, organizations, products, concepts) and all triplets (subject, predicate, object) describing their relationships.\n",
        "\n",
        "Statement: \"{statement}\"\n",
        "\n",
        "Predicate list:\n",
        "{predicate_instructions}\n",
        "\n",
        "Guidelines:\n",
        "- List entities with unique `entity_idx`.\n",
        "- List triplets linking subjects and objects by `entity_idx`.\n",
        "- Exclude temporal expressions from entities and triplets.\n",
        "\n",
        "Example:\n",
        "Statement: \"Google's revenue increased by 10% from January through March.\"\n",
        "Output: {{\n",
        "  \"entities\": [\n",
        "    {{\"entity_idx\": 0, \"name\": \"Google\", \"type\": \"Organization\", \"description\": \"A multinational technology company.\"}},\n",
        "    {{\"entity_idx\": 1, \"name\": \"Revenue\", \"type\": \"Financial Metric\", \"description\": \"Income from normal business.\"}}\n",
        "  ],\n",
        "  \"triplets\": [\n",
        "    {{\"subject_name\": \"Google\", \"subject_id\": 0, \"predicate\": \"INCREASED\", \"object_name\": \"Revenue\", \"object_id\": 1, \"value\": \"10%\"}}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Return ONLY a valid JSON object matching `RawExtraction`.\n",
        "\"\"\"\n",
        "\n",
        "# Initializing the prompt template\n",
        "triplet_extraction_prompt = ChatPromptTemplate.from_template(triplet_extraction_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYR2jY96s_dv",
        "outputId": "334b4e92-10d1-47e6-b5ce-e5cd8ee5bdc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running triplet extraction for statement ---\n",
            "Statement: \"AMD has been very focused on the server launch for the first half of 2017.\"\n",
            "\n",
            "--- Triplet Extraction Result ---\n",
            "{\n",
            "  \"entities\": [\n",
            "    {\n",
            "      \"entity_idx\": 0,\n",
            "      \"name\": \"AMD\",\n",
            "      \"type\": \"Organization\",\n",
            "      \"description\": \"A multinational semiconductor company.\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_idx\": 1,\n",
            "      \"name\": \"server launch\",\n",
            "      \"type\": \"Event\",\n",
            "      \"description\": \"The release of server-related products.\"\n",
            "    }\n",
            "  ],\n",
            "  \"triplets\": [\n",
            "    {\n",
            "      \"subject_name\": \"AMD\",\n",
            "      \"subject_id\": 0,\n",
            "      \"predicate\": \"HAS_A\",\n",
            "      \"object_name\": \"server launch\",\n",
            "      \"object_id\": 1,\n",
            "      \"value\": null\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Create the chain for triplet and entity extraction.\n",
        "triplet_extraction_chain = triplet_extraction_prompt | llm.with_structured_output(RawExtraction)\n",
        "\n",
        "# Let's use the same statement we've been working with.\n",
        "sample_statement_for_triplets = extracted_statements_list.statements[0]\n",
        "\n",
        "print(f\"--- Running triplet extraction for statement ---\")\n",
        "print(f'Statement: \"{sample_statement_for_triplets.statement}\"')\n",
        "\n",
        "# Invoke the chain.\n",
        "raw_extraction_result = triplet_extraction_chain.invoke({\n",
        "    \"statement\": sample_statement_for_triplets.statement,\n",
        "    \"predicate_instructions\": predicate_instructions_text\n",
        "})\n",
        "\n",
        "print(\"\\n--- Triplet Extraction Result ---\")\n",
        "print(raw_extraction_result.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assembling the Temporal Event"
      ],
      "metadata": {
        "id": "UsVOx54Q1gEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This stage then merges the statements, dates and entities and triplets into a single object called a Temporal Event. This acts as the master record that holds the original statement, type (fact, dynamic etc), validity dates, associated triplets and metadata. The temporal event assigns a UUID (universally unique identifier) as its main identifier so each fact can be tracked across the knowledge graph."
      ],
      "metadata": {
        "id": "wzIHTV_W1kmr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQAQMaKntBqo"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Final persistent model for an entity in your knowledge graph\n",
        "class Entity(BaseModel):\n",
        "    id: uuid.UUID = Field(default_factory=uuid.uuid4, description=\"Unique UUID for the entity\")\n",
        "    name: str = Field(..., description=\"The name of the entity\")\n",
        "    type: str = Field(..., description=\"Entity type, e.g., 'Organization', 'Person'\")\n",
        "    description: str = Field(\"\", description=\"Brief description of the entity\")\n",
        "    resolved_id: uuid.UUID | None = Field(None, description=\"UUID of resolved entity if merged\")\n",
        "\n",
        "# Final persistent model for a triplet relationship\n",
        "class Triplet(BaseModel):\n",
        "    id: uuid.UUID = Field(default_factory=uuid.uuid4, description=\"Unique UUID for the triplet\")\n",
        "    subject_name: str = Field(..., description=\"Name of the subject entity\")\n",
        "    subject_id: uuid.UUID = Field(..., description=\"UUID of the subject entity\")\n",
        "    predicate: Predicate = Field(..., description=\"Relationship predicate\")\n",
        "    object_name: str = Field(..., description=\"Name of the object entity\")\n",
        "    object_id: uuid.UUID = Field(..., description=\"UUID of the object entity\")\n",
        "    value: str | None = Field(None, description=\"Optional value associated with the triplet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCr7MMRotIhR"
      },
      "outputs": [],
      "source": [
        "class TemporalEvent(BaseModel):\n",
        "    \"\"\"\n",
        "    The central model that consolidates all extracted information.\n",
        "    \"\"\"\n",
        "    id: uuid.UUID = Field(default_factory=uuid.uuid4)\n",
        "    chunk_id: uuid.UUID # To link back to the original text chunk\n",
        "    statement: str\n",
        "    embedding: list[float] = [] # For similarity checks later\n",
        "\n",
        "    # Information from our previous extraction steps\n",
        "    statement_type: StatementType\n",
        "    temporal_type: TemporalType\n",
        "    valid_at: datetime | None = None\n",
        "    invalid_at: datetime | None = None\n",
        "\n",
        "    # A list of the IDs of the triplets associated with this event\n",
        "    triplets: list[uuid.UUID]\n",
        "\n",
        "    # Extra metadata for tracking changes over time\n",
        "    created_at: datetime = Field(default_factory=datetime.now)\n",
        "    expired_at: datetime | None = None\n",
        "    invalidated_by: uuid.UUID | None = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKq_kj7CtKxY",
        "outputId": "8d07865b-ae45-4a5e-96c3-2135bef49aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Assembling the final TemporalEvent ---\n",
            "Created 2 persistent Entity objects.\n",
            "Created 1 persistent Triplet objects.\n"
          ]
        }
      ],
      "source": [
        "# Assume these are already defined from previous steps:\n",
        "# sample_statement, final_temporal_range, raw_extraction_result\n",
        "\n",
        "print(\"--- Assembling the final TemporalEvent ---\")\n",
        "\n",
        "# 1. Convert raw entities to persistent Entity objects with UUIDs\n",
        "idx_to_entity_map: dict[int, Entity] = {}\n",
        "final_entities: list[Entity] = []\n",
        "\n",
        "for raw_entity in raw_extraction_result.entities:\n",
        "    entity = Entity(\n",
        "        name=raw_entity.name,\n",
        "        type=raw_entity.type,\n",
        "        description=raw_entity.description\n",
        "    )\n",
        "    idx_to_entity_map[raw_entity.entity_idx] = entity\n",
        "    final_entities.append(entity)\n",
        "\n",
        "print(f\"Created {len(final_entities)} persistent Entity objects.\")\n",
        "\n",
        "# 2. Convert raw triplets to persistent Triplet objects, linking entities via UUIDs\n",
        "final_triplets: list[Triplet] = []\n",
        "\n",
        "for raw_triplet in raw_extraction_result.triplets:\n",
        "    subject_entity = idx_to_entity_map[raw_triplet.subject_id]\n",
        "    object_entity = idx_to_entity_map[raw_triplet.object_id]\n",
        "\n",
        "    triplet = Triplet(\n",
        "        subject_name=subject_entity.name,\n",
        "        subject_id=subject_entity.id,\n",
        "        predicate=raw_triplet.predicate,\n",
        "        object_name=object_entity.name,\n",
        "        object_id=object_entity.id,\n",
        "        value=raw_triplet.value\n",
        "    )\n",
        "    final_triplets.append(triplet)\n",
        "\n",
        "print(f\"Created {len(final_triplets)} persistent Triplet objects.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhXIccf_tRsC",
        "outputId": "5f789764-f98b-46d1-89c2-68c613cf523d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Final Assembled TemporalEvent ---\n",
            "{\n",
            "  \"id\": \"d7a2bc93-f1c2-4356-8281-4c64948b1a6b\",\n",
            "  \"chunk_id\": \"f4669226-e138-4bd4-a4e2-e6a8393fdaea\",\n",
            "  \"statement\": \"AMD has been very focused on the server launch for the first half of 2017.\",\n",
            "  \"embedding\": [],\n",
            "  \"statement_type\": \"FACT\",\n",
            "  \"temporal_type\": \"DYNAMIC\",\n",
            "  \"valid_at\": \"2017-01-01T00:00:00Z\",\n",
            "  \"invalid_at\": \"2017-06-30T23:59:59Z\",\n",
            "  \"triplets\": [\n",
            "    \"6d9294df-9693-4c47-875f-49d08083289c\"\n",
            "  ],\n",
            "  \"created_at\": \"2025-08-16T21:07:15.176944\",\n",
            "  \"expired_at\": null,\n",
            "  \"invalidated_by\": null\n",
            "}\n",
            "\n",
            "--- Associated Entities ---\n",
            "{\n",
            "  \"id\": \"ba639b08-b69e-4c9b-9189-643a5decac14\",\n",
            "  \"name\": \"AMD\",\n",
            "  \"type\": \"Organization\",\n",
            "  \"description\": \"A multinational semiconductor company.\",\n",
            "  \"resolved_id\": null\n",
            "}\n",
            "{\n",
            "  \"id\": \"b93aa6ba-3868-4bf6-857b-658049af64e2\",\n",
            "  \"name\": \"server launch\",\n",
            "  \"type\": \"Event\",\n",
            "  \"description\": \"The release of server-related products.\",\n",
            "  \"resolved_id\": null\n",
            "}\n",
            "\n",
            "--- Associated Triplets ---\n",
            "{\n",
            "  \"id\": \"6d9294df-9693-4c47-875f-49d08083289c\",\n",
            "  \"subject_name\": \"AMD\",\n",
            "  \"subject_id\": \"ba639b08-b69e-4c9b-9189-643a5decac14\",\n",
            "  \"predicate\": \"HAS_A\",\n",
            "  \"object_name\": \"server launch\",\n",
            "  \"object_id\": \"b93aa6ba-3868-4bf6-857b-658049af64e2\",\n",
            "  \"value\": null\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# 3. Create the final TemporalEvent object\n",
        "# We'll generate a dummy chunk_id for this example.\n",
        "temporal_event = TemporalEvent(\n",
        "    chunk_id=uuid.uuid4(), # Placeholder ID\n",
        "    statement=sample_statement.statement,\n",
        "    statement_type=sample_statement.statement_type,\n",
        "    temporal_type=sample_statement.temporal_type,\n",
        "    valid_at=final_temporal_range.valid_at,\n",
        "    invalid_at=final_temporal_range.invalid_at,\n",
        "    triplets=[t.id for t in final_triplets]\n",
        ")\n",
        "\n",
        "print(\"\\n--- Final Assembled TemporalEvent ---\")\n",
        "print(temporal_event.model_dump_json(indent=2))\n",
        "\n",
        "print(\"\\n--- Associated Entities ---\")\n",
        "for entity in final_entities:\n",
        "    print(entity.model_dump_json(indent=2))\n",
        "\n",
        "print(\"\\n--- Associated Triplets ---\")\n",
        "for triplet in final_triplets:\n",
        "    print(triplet.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automating the Pipeline with LangGraph"
      ],
      "metadata": {
        "id": "nvI9t4VF12Lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up to now the statements, dates and triplets were extracted manually and assembled into TemporalEvents - to scale this across thousands of chunks the process is automated using LangGraph (which is a library for building Al workflows as directed graphs). With LangGraph, each stage like extracting statements, pulling out dates and building triplets becomes a separate node in a graph. The workflow is defined with a start point and end point to ensure we can scale across all the document chunks quickly. In this case one run, 19 chunks produced 95 structured temporal events, 213 entities and 121 triplets."
      ],
      "metadata": {
        "id": "Kw-F7A5P5rgv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-87-av2tUN-"
      },
      "outputs": [],
      "source": [
        "from typing import List, TypedDict\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    TypedDict representing the overall state of the knowledge graph ingestion.\n",
        "\n",
        "    Attributes:\n",
        "        chunks: List of Document chunks being processed.\n",
        "        temporal_events: List of TemporalEvent objects extracted from chunks.\n",
        "        entities: List of Entity objects in the graph.\n",
        "        triplets: List of Triplet objects representing relationships.\n",
        "    \"\"\"\n",
        "    chunks: List[Document]\n",
        "    temporal_events: List[TemporalEvent]\n",
        "    entities: List[Entity]\n",
        "    triplets: List[Triplet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELIK0SpztX9g"
      },
      "outputs": [],
      "source": [
        "def extract_events_from_chunks(state: GraphState) -> GraphState:\n",
        "    chunks = state[\"chunks\"]\n",
        "\n",
        "    # Extract raw statements from each chunk\n",
        "    raw_stmts = statement_extraction_chain.batch([{\n",
        "        \"main_entity\": c.metadata[\"company\"],\n",
        "        \"publication_date\": c.metadata[\"date\"].isoformat(),\n",
        "        \"document_chunk\": c.page_content,\n",
        "        \"definitions\": definitions_text\n",
        "    } for c in chunks])\n",
        "\n",
        "    # Flatten statements, attach metadata and unique chunk IDs\n",
        "    stmts = [{\"raw\": s, \"meta\": chunks[i].metadata, \"cid\": uuid.uuid4()}\n",
        "             for i, rs in enumerate(raw_stmts) for s in rs.statements]\n",
        "\n",
        "    # Prepare inputs and batch extract temporal data\n",
        "    dates = date_extraction_chain.batch([{\n",
        "        \"statement\": s[\"raw\"].statement,\n",
        "        \"statement_type\": s[\"raw\"].statement_type.value,\n",
        "        \"temporal_type\": s[\"raw\"].temporal_type.value,\n",
        "        \"publication_date\": s[\"meta\"][\"date\"].isoformat(),\n",
        "        \"quarter\": s[\"meta\"][\"quarter\"]\n",
        "    } for s in stmts])\n",
        "\n",
        "    # Prepare inputs and batch extract triplets\n",
        "    trips = triplet_extraction_chain.batch([{\n",
        "        \"statement\": s[\"raw\"].statement,\n",
        "        \"predicate_instructions\": predicate_instructions_text\n",
        "    } for s in stmts])\n",
        "\n",
        "    events, entities, triplets = [], [], []\n",
        "\n",
        "    for i, s in enumerate(stmts):\n",
        "        # Validate temporal range data\n",
        "        tr = TemporalValidityRange.model_validate(dates[i].model_dump())\n",
        "        ext = trips[i]\n",
        "\n",
        "        # Map entities by index and collect them\n",
        "        idx_map = {e.entity_idx: Entity(e.name, e.type, e.description) for e in ext.entities}\n",
        "        entities.extend(idx_map.values())\n",
        "\n",
        "        # Build triplets only if subject and object entities exist\n",
        "        tpls = [Triplet(\n",
        "            idx_map[t.subject_id].name, idx_map[t.subject_id].id, t.predicate,\n",
        "            idx_map[t.object_id].name, idx_map[t.object_id].id, t.value)\n",
        "            for t in ext.triplets if t.subject_id in idx_map and t.object_id in idx_map]\n",
        "        triplets.extend(tpls)\n",
        "\n",
        "        # Create TemporalEvent with linked triplet IDs\n",
        "        events.append(TemporalEvent(\n",
        "            chunk_id=s[\"cid\"], statement=s[\"raw\"].statement,\n",
        "            statement_type=s[\"raw\"].statement_type, temporal_type=s[\"raw\"].temporal_type,\n",
        "            valid_at=tr.valid_at, invalid_at=tr.invalid_at,\n",
        "            triplets=[t.id for t in tpls]\n",
        "        ))\n",
        "\n",
        "    return {\"chunks\": chunks, \"temporal_events\": events, \"entities\": entities, \"triplets\": triplets}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3F0rU95t7El",
        "outputId": "af8eaf31-a726-4409-e0dc-43d26aa36ec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.6.5-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.74)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.0 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.7)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m865.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (3.11.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.32.3)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.6.5-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.2/153.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.6.5 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.0 ormsgpack-1.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwY-4sI5tbO6"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Define a new graph using our state\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add our function as a node named \"extract_events\"\n",
        "workflow.add_node(\"extract_events\", extract_events_from_chunks)\n",
        "\n",
        "# Define the starting point of the graph\n",
        "workflow.set_entry_point(\"extract_events\")\n",
        "\n",
        "# Define the end point of the graph\n",
        "workflow.add_edge(\"extract_events\", END)\n",
        "\n",
        "# Compile the graph into a runnable application\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VRCk8jA7tfCW",
        "outputId": "c0dd25c4-40cb-48c5-8397-387a86ac767a"
      },
      "outputs": [
        {
          "ename": "APIStatusError",
          "evalue": "Error code: 402 - {'detail': 'Payment Required: You have exhausted your budget. Please add funds to continue using the API.'}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-261577379.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Invoke the graph. This will run our entire extraction pipeline in one call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3024\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3027\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2648\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    163\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3018860863.py\u001b[0m in \u001b[0;36mextract_events_from_chunks\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Extract raw statements from each chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     raw_stmts = statement_extraction_chain.batch([{\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;34m\"main_entity\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"company\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;34m\"publication_date\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mbatch\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m   3196\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3197\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3198\u001b[0;31m                     inputs = step.batch(\n\u001b[0m\u001b[1;32m   3199\u001b[0m                         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3200\u001b[0m                         [\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mbatch\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m   5474\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5475\u001b[0m             \u001b[0mconfigs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5476\u001b[0;31m         return self.bound.batch(\n\u001b[0m\u001b[1;32m   5477\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5478\u001b[0m             \u001b[0mconfigs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mbatch\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mget_executor_for_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"list[Output]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    617\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36m_wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         return super().map(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(input_, config)\u001b[0m\n\u001b[1;32m    789\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;31m# If there's only one input, don't bother with the executor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m         return cast(\n\u001b[1;32m    382\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1005\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m                 results.append(\n\u001b[0;32m--> 825\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    826\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1073\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0mgeneration_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1148\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1149\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1151\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAPIStatusError\u001b[0m: Error code: 402 - {'detail': 'Payment Required: You have exhausted your budget. Please add funds to continue using the API.'}"
          ]
        }
      ],
      "source": [
        "# The input is a dictionary matching our GraphState, providing the initial chunks\n",
        "graph_input = {\"chunks\": chunked_documents_lc}\n",
        "\n",
        "# Invoke the graph. This will run our entire extraction pipeline in one call.\n",
        "final_state = app.invoke(graph_input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cURf7gliuAKi"
      },
      "outputs": [],
      "source": [
        "# Check the number of objects created in the final state\n",
        "num_events = len(final_state['temporal_events'])\n",
        "num_entities = len(final_state['entities'])\n",
        "num_triplets = len(final_state['triplets'])\n",
        "\n",
        "print(f\"Total TemporalEvents created: {num_events}\")\n",
        "print(f\"Total Entities created: {num_entities}\")\n",
        "print(f\"Total Triplets created: {num_triplets}\")\n",
        "\n",
        "print(\"\\n--- Sample TemporalEvent from the final state ---\")\n",
        "# Print a sample event to see the fully assembled object\n",
        "print(final_state['temporal_events'][5].model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning Our Data with Entity Resolution"
      ],
      "metadata": {
        "id": "khgbQNJF6kDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the next stage, dealing with the same entity that comes up with multiple names, we use entity resolution. This means clustering together different mentions and assigning them to one clean ID and adding them to a new LangGraph node (1) group entities by type (person/company) (2) use fuzzy string matching to detect duplicates (3) pick a clean version for each cluster (4) update all triplets using the clean ID instead of duplicates — we store the clean entity names in a SQL database which uses the new entity resolution."
      ],
      "metadata": {
        "id": "JYaHDJyJ6ecC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NW2489zbuECB"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "\n",
        "def setup_in_memory_db():\n",
        "    \"\"\"\n",
        "    Sets up an in-memory SQLite database and creates the 'entities' table.\n",
        "\n",
        "    The 'entities' table schema:\n",
        "    - id: TEXT, Primary Key\n",
        "    - name: TEXT, name of the entity\n",
        "    - type: TEXT, type/category of the entity\n",
        "    - description: TEXT, description of the entity\n",
        "    - is_canonical: INTEGER, flag to indicate if entity is canonical (default 1)\n",
        "\n",
        "    Returns:\n",
        "        sqlite3.Connection: A connection object to the in-memory database.\n",
        "    \"\"\"\n",
        "    # Establish connection to an in-memory SQLite database\n",
        "    conn = sqlite3.connect(\":memory:\")\n",
        "\n",
        "    # Create a cursor object to execute SQL commands\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Create the 'entities' table if it doesn't already exist\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS entities (\n",
        "            id TEXT PRIMARY KEY,\n",
        "            name TEXT,\n",
        "            type TEXT,\n",
        "            description TEXT,\n",
        "            is_canonical INTEGER DEFAULT 1\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    # Commit changes to save the table schema\n",
        "    conn.commit()\n",
        "\n",
        "    # Return the connection object for further use\n",
        "    return conn\n",
        "\n",
        "# Create the database connection and set up the entities table\n",
        "db_conn = setup_in_memory_db()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efCXqVDyuJL-"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from rapidfuzz import fuzz\n",
        "from collections import defaultdict\n",
        "\n",
        "def resolve_entities_in_state(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    A LangGraph node to perform entity resolution on the extracted entities.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Entering Node: resolve_entities_in_state ---\")\n",
        "    entities = state[\"entities\"]\n",
        "    triplets = state[\"triplets\"]\n",
        "\n",
        "    cursor = db_conn.cursor()\n",
        "    cursor.execute(\"SELECT id, name FROM entities WHERE is_canonical = 1\")\n",
        "    global_canonicals = {row[1]: uuid.UUID(row[0]) for row in cursor.fetchall()}\n",
        "\n",
        "    print(f\"Starting resolution with {len(entities)} entities. Found {len(global_canonicals)} canonicals in DB.\")\n",
        "\n",
        "    # Group entities by type (e.g., 'Person', 'Organization') for more accurate matching\n",
        "    type_groups = defaultdict(list)\n",
        "    for entity in entities:\n",
        "        type_groups[entity.type].append(entity)\n",
        "\n",
        "    resolved_id_map = {} # Maps an old entity ID to its new canonical ID\n",
        "    newly_created_canonicals = {}\n",
        "\n",
        "    for entity_type, group in type_groups.items():\n",
        "        if not group: continue\n",
        "\n",
        "        # Cluster entities in the group by fuzzy name matching\n",
        "        clusters = []\n",
        "        used_indices = set()\n",
        "        for i in range(len(group)):\n",
        "            if i in used_indices: continue\n",
        "            current_cluster = [group[i]]\n",
        "            used_indices.add(i)\n",
        "            for j in range(i + 1, len(group)):\n",
        "                if j in used_indices: continue\n",
        "                # Use partial_ratio for flexible matching (e.g., \"AMD\" vs \"Advanced Micro Devices, Inc.\")\n",
        "                score = fuzz.partial_ratio(group[i].name.lower(), group[j].name.lower())\n",
        "                if score >= 80.0: # A similarity threshold of 80%\n",
        "                    current_cluster.append(group[j])\n",
        "                    used_indices.add(j)\n",
        "            clusters.append(current_cluster)\n",
        "\n",
        "        # For each cluster, find the best canonical representation (the \"medoid\")\n",
        "        for cluster in clusters:\n",
        "            scores = {e.name: sum(fuzz.ratio(e.name.lower(), other.name.lower()) for other in cluster) for e in cluster}\n",
        "            medoid_entity = max(cluster, key=lambda e: scores[e.name])\n",
        "            canonical_name = medoid_entity.name\n",
        "\n",
        "            # Check if this canonical name already exists or was just created in this run\n",
        "            if canonical_name in global_canonicals:\n",
        "                canonical_id = global_canonicals[canonical_name]\n",
        "            elif canonical_name in newly_created_canonicals:\n",
        "                canonical_id = newly_created_canonicals[canonical_name].id\n",
        "            else:\n",
        "                # Create a new canonical entity\n",
        "                canonical_id = medoid_entity.id\n",
        "                newly_created_canonicals[canonical_name] = medoid_entity\n",
        "\n",
        "            # Map all entities in this cluster to the single canonical ID\n",
        "            for entity in cluster:\n",
        "                entity.resolved_id = canonical_id\n",
        "                resolved_id_map[entity.id] = canonical_id\n",
        "\n",
        "    # Update the triplets in our state to use the new canonical IDs\n",
        "    for triplet in triplets:\n",
        "        if triplet.subject_id in resolved_id_map:\n",
        "            triplet.subject_id = resolved_id_map[triplet.subject_id]\n",
        "        if triplet.object_id in resolved_id_map:\n",
        "            triplet.object_id = resolved_id_map[triplet.object_id]\n",
        "\n",
        "    # Add any newly created canonical entities to our database\n",
        "    if newly_created_canonicals:\n",
        "        print(f\"Adding {len(newly_created_canonicals)} new canonical entities to the DB.\")\n",
        "        new_data = [(str(e.id), e.name, e.type, e.description, 1) for e in newly_created_canonicals.values()]\n",
        "        cursor.executemany(\"INSERT INTO entities (id, name, type, description, is_canonical) VALUES (?, ?, ?, ?, ?)\", new_data)\n",
        "        db_conn.commit()\n",
        "\n",
        "    print(\"Entity resolution complete.\")\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-crazFbxuL7d"
      },
      "outputs": [],
      "source": [
        "# Re-define the graph to include the new node\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add our two nodes to the graph\n",
        "workflow.add_node(\"extract_events\", extract_events_from_chunks)\n",
        "workflow.add_node(\"resolve_entities\", resolve_entities_in_state)\n",
        "\n",
        "# Define the new sequence of steps\n",
        "workflow.set_entry_point(\"extract_events\")\n",
        "workflow.add_edge(\"extract_events\", \"resolve_entities\")\n",
        "workflow.add_edge(\"resolve_entities\", END)\n",
        "\n",
        "# Compile the updated workflow\n",
        "app_with_resolution = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULL_GVH9uN4e"
      },
      "outputs": [],
      "source": [
        "# Use the same input as before\n",
        "graph_input = {\"chunks\": chunked_documents_lc}\n",
        "\n",
        "# Invoke the new graph\n",
        "final_state_with_resolution = app_with_resolution.invoke(graph_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHkzilvcuVWF"
      },
      "outputs": [],
      "source": [
        "# Find a sample entity that has been resolved (i.e., has a resolved_id)\n",
        "sample_resolved_entity = next((e for e in final_state_with_resolution['entities'] if e.resolved_id is not None and e.id != e.resolved_id), None)\n",
        "\n",
        "if sample_resolved_entity:\n",
        "    print(\"\\n--- Sample of a Resolved Entity ---\")\n",
        "    print(sample_resolved_entity.model_dump_json(indent=2))\n",
        "else:\n",
        "    print(\"\\nNo sample resolved entity found (all entities were unique in this small run).\")\n",
        "\n",
        "# Check a triplet to see its updated canonical IDs\n",
        "sample_resolved_triplet = final_state_with_resolution['triplets'][0]\n",
        "print(\"\\n--- Sample Triplet with Resolved IDs ---\")\n",
        "print(sample_resolved_triplet.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making Our Knowledge Dynamic with an Invalidation Agent"
      ],
      "metadata": {
        "id": "s3i92Clr66ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The biggest challenge is that facts change over time - which is now where the invalidation agent comes in. Its job is to act like a referee: it spots contradictions between old and new dynamic facts and if something has changes, it updates the old facts with an invalid at timestamp. The process works like (1) embedding all statements to measure semantic similarity (2) comparing new dynamic facts against existing ones (3) using an LLM to judge whether the new fact invalidates the old (4) if so, mark the old fact as expired. This ensures the knowledge base is dynamic and time-aware."
      ],
      "metadata": {
        "id": "9qSocP3769DS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdbWZ7BOuZ3Z"
      },
      "outputs": [],
      "source": [
        "# Obtain a cursor from the existing database connection\n",
        "cursor = db_conn.cursor()\n",
        "\n",
        "# Create the 'events' table to store event-related data\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS events (\n",
        "    id TEXT PRIMARY KEY,         -- Unique identifier for each event\n",
        "    chunk_id TEXT,               -- Identifier for the chunk this event belongs to\n",
        "    statement TEXT,              -- Textual representation of the event\n",
        "    statement_type TEXT,         -- Type/category of the statement (e.g., assertion, question)\n",
        "    temporal_type TEXT,          -- Temporal classification (e.g., past, present, future)\n",
        "    valid_at TEXT,               -- Timestamp when the event becomes valid\n",
        "    invalid_at TEXT,             -- Timestamp when the event becomes invalid\n",
        "    embedding BLOB               -- Optional embedding data stored as binary (e.g., vector)\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "# Create the 'triplets' table to store relations between entities for events\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS triplets (\n",
        "    id TEXT PRIMARY KEY,         -- Unique identifier for each triplet\n",
        "    event_id TEXT,               -- Foreign key referencing 'events.id'\n",
        "    subject_id TEXT,             -- Subject entity ID in the triplet\n",
        "    predicate TEXT               -- Predicate describing relation or action\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "# Commit all changes to the in-memory database\n",
        "db_conn.commit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfTi-0KWua0C"
      },
      "outputs": [],
      "source": [
        "# This prompt asks the LLM to act as a referee between two events.\n",
        "event_invalidation_prompt_template = \"\"\"\n",
        "Task: Analyze the primary event against the secondary event and determine if the primary event is invalidated by the secondary event.\n",
        "Return \"True\" if the primary event is invalidated, otherwise return \"False\".\n",
        "\n",
        "Invalidation Guidelines:\n",
        "1. An event can only be invalidated if it is DYNAMIC and its `invalid_at` is currently null.\n",
        "2. A STATIC event (e.g., \"X was hired on date Y\") can invalidate a DYNAMIC event (e.g., \"Z is the current employee\").\n",
        "3. Invalidation must be a direct contradiction. For example, \"Lisa Su is CEO\" is contradicted by \"Someone else is CEO\".\n",
        "4. The invalidating event (secondary) must occur at or after the start of the primary event.\n",
        "\n",
        "---\n",
        "Primary Event (the one that might be invalidated):\n",
        "- Statement: {primary_statement}\n",
        "- Type: {primary_temporal_type}\n",
        "- Valid From: {primary_valid_at}\n",
        "- Valid To: {primary_invalid_at}\n",
        "\n",
        "Secondary Event (the new fact that might cause invalidation):\n",
        "- Statement: {secondary_statement}\n",
        "- Type: {secondary_temporal_type}\n",
        "- Valid From: {secondary_valid_at}\n",
        "---\n",
        "\n",
        "Is the primary event invalidated by the secondary event? Answer with only \"True\" or \"False\".\n",
        "\"\"\"\n",
        "\n",
        "invalidation_prompt = ChatPromptTemplate.from_template(event_invalidation_prompt_template)\n",
        "\n",
        "# This chain will output a simple string: \"True\" or \"False\".\n",
        "invalidation_chain = invalidation_prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGoUjTHKuftF"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def invalidate_events_in_state(state: GraphState) -> GraphState:\n",
        "    \"\"\"Mark dynamic events invalidated by later similar facts.\"\"\"\n",
        "    events = state[\"temporal_events\"]\n",
        "\n",
        "    # Embed all event statements\n",
        "    embeds = embeddings.embed_documents([e.statement for e in events])\n",
        "    for e, emb in zip(events, embeds):\n",
        "        e.embedding = emb\n",
        "\n",
        "    updates = {}\n",
        "    for i, e1 in enumerate(events):\n",
        "        # Skip non-dynamic or already invalidated events\n",
        "        if e1.temporal_type != TemporalType.DYNAMIC or e1.invalid_at:\n",
        "            continue\n",
        "\n",
        "        # Find candidate events: facts starting at or after e1 with high similarity\n",
        "        cands = [\n",
        "            e2 for j, e2 in enumerate(events) if j != i and\n",
        "            e2.statement_type == StatementType.FACT and e2.valid_at and e1.valid_at and\n",
        "            e2.valid_at >= e1.valid_at and 1 - cosine(e1.embedding, e2.embedding) > 0.5\n",
        "        ]\n",
        "        if not cands:\n",
        "            continue\n",
        "\n",
        "        # Prepare inputs for LLM invalidation check\n",
        "        inputs = [{\n",
        "            \"primary_statement\": e1.statement, \"primary_temporal_type\": e1.temporal_type.value,\n",
        "            \"primary_valid_at\": e1.valid_at.isoformat(), \"primary_invalid_at\": \"None\",\n",
        "            \"secondary_statement\": c.statement, \"secondary_temporal_type\": c.temporal_type.value,\n",
        "            \"secondary_valid_at\": c.valid_at.isoformat()\n",
        "        } for c in cands]\n",
        "\n",
        "        # Ask LLM which candidates invalidate the event\n",
        "        results = invalidation_chain.batch(inputs)\n",
        "\n",
        "        # Record earliest invalidation info\n",
        "        for c, r in zip(cands, results):\n",
        "            if r.content.strip().lower() == \"true\" and (e1.id not in updates or c.valid_at < updates[e1.id][\"invalid_at\"]):\n",
        "                updates[e1.id] = {\"invalid_at\": c.valid_at, \"invalidated_by\": c.id}\n",
        "\n",
        "    # Apply invalidations to events\n",
        "    for e in events:\n",
        "        if e.id in updates:\n",
        "            e.invalid_at = updates[e.id][\"invalid_at\"]\n",
        "            e.invalidated_by = updates[e.id][\"invalidated_by\"]\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy5Ccjxmuh1u"
      },
      "outputs": [],
      "source": [
        "# Re-define the graph to include all three nodes\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "workflow.add_node(\"extract_events\", extract_events_from_chunks)\n",
        "workflow.add_node(\"resolve_entities\", resolve_entities_in_state)\n",
        "workflow.add_node(\"invalidate_events\", invalidate_events_in_state)\n",
        "\n",
        "# Define the complete pipeline flow\n",
        "workflow.set_entry_point(\"extract_events\")\n",
        "workflow.add_edge(\"extract_events\", \"resolve_entities\")\n",
        "workflow.add_edge(\"resolve_entities\", \"invalidate_events\")\n",
        "workflow.add_edge(\"invalidate_events\", END)\n",
        "\n",
        "# Compile the final ingestion workflow\n",
        "ingestion_app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XsrDX9cuifd"
      },
      "outputs": [],
      "source": [
        "# Use the same input as before\n",
        "graph_input = {\"chunks\": chunked_documents_lc}\n",
        "\n",
        "# Invoke the final graph\n",
        "final_ingested_state = ingestion_app.invoke(graph_input)\n",
        "print(\"\\n--- Full graph execution with invalidation complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OShOPeSNutBy"
      },
      "outputs": [],
      "source": [
        "# Find and print an invalidated event from the final state\n",
        "invalidated_event = next((e for e in final_ingested_state['temporal_events'] if e.invalidated_by is not None), None)\n",
        "\n",
        "if invalidated_event:\n",
        "    print(\"\\n--- Sample of an Invalidated Event ---\")\n",
        "    print(invalidated_event.model_dump_json(indent=2))\n",
        "\n",
        "    # Find the event that caused the invalidation\n",
        "    invalidating_event = next((e for e in final_ingested_state['temporal_events'] if e.id == invalidated_event.invalidated_by), None)\n",
        "\n",
        "    if invalidating_event:\n",
        "        print(\"\\n--- Was Invalidated By this Event ---\")\n",
        "        print(invalidating_event.model_dump_json(indent=2))\n",
        "else:\n",
        "    print(\"\\nNo invalidated events were found in this run.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assembling the Temporal Knowledge Graph"
      ],
      "metadata": {
        "id": "ky8lm9pK7goR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we've done the hard work of extracting facts, resolving duplicates and handling invalidations. Now its time to assemble the temporal knowledge graph. So the entities become the nodes, triplets (relationships) become the edges connecting them and each edge carries rich temporal information - when it becomes valid, when it expires and the statement behind it. We use network for this, to build this structure directly from the final LangGraph pipeline output. As you cans ee, we end up with a graph with 340 nodes and 434 edges, where you can zoom in on company and instantly see the relationships over time. This temporal knowledge graph essentially becomes the 'brain' that our smart retrieval agent will query to answer nuanced and time-sensitive questions."
      ],
      "metadata": {
        "id": "YxYCp5NE7gdJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE_i_n9pu0WO"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import uuid\n",
        "\n",
        "def build_graph_from_state(state: GraphState) -> nx.MultiDiGraph:\n",
        "    \"\"\"\n",
        "    Builds a NetworkX graph from the final state of our ingestion pipeline.\n",
        "    \"\"\"\n",
        "    print(\"--- Building Knowledge Graph from final state ---\")\n",
        "\n",
        "    entities = state[\"entities\"]\n",
        "    triplets = state[\"triplets\"]\n",
        "    temporal_events = state[\"temporal_events\"]\n",
        "\n",
        "    # Create a quick-lookup map from an entity's ID to the entity object itself\n",
        "    entity_map = {entity.id: entity for entity in entities}\n",
        "\n",
        "    graph = nx.MultiDiGraph() # A directed graph that allows multiple edges\n",
        "\n",
        "    # 1. Add a node for each unique, canonical entity\n",
        "    canonical_ids = {e.resolved_id if e.resolved_id else e.id for e in entities}\n",
        "    for canonical_id in canonical_ids:\n",
        "        # Find the entity object that represents this canonical ID\n",
        "        canonical_entity_obj = entity_map.get(canonical_id)\n",
        "        if canonical_entity_obj:\n",
        "            graph.add_node(\n",
        "                str(canonical_id), # Node names in NetworkX are typically strings\n",
        "                name=canonical_entity_obj.name,\n",
        "                type=canonical_entity_obj.type,\n",
        "                description=canonical_entity_obj.description\n",
        "            )\n",
        "\n",
        "    print(f\"Added {graph.number_of_nodes()} canonical entity nodes to the graph.\")\n",
        "\n",
        "    # 2. Add an edge for each triplet, decorated with temporal info\n",
        "    edges_added = 0\n",
        "    event_map = {event.id: event for event in temporal_events}\n",
        "    for triplet in triplets:\n",
        "        # Find the parent event that this triplet belongs to\n",
        "        parent_event = next((ev for ev in temporal_events if triplet.id in ev.triplets), None)\n",
        "        if not parent_event: continue\n",
        "\n",
        "        # Get the canonical IDs for the subject and object\n",
        "        subject_canonical_id = str(triplet.subject_id)\n",
        "        object_canonical_id = str(triplet.object_id)\n",
        "\n",
        "        # Add the edge to the graph\n",
        "        if graph.has_node(subject_canonical_id) and graph.has_node(object_canonical_id):\n",
        "            edge_attrs = {\n",
        "                \"predicate\": triplet.predicate.value, \"value\": triplet.value,\n",
        "                \"statement\": parent_event.statement, \"valid_at\": parent_event.valid_at,\n",
        "                \"invalid_at\": parent_event.invalid_at,\n",
        "                \"statement_type\": parent_event.statement_type.value\n",
        "            }\n",
        "            graph.add_edge(\n",
        "                subject_canonical_id, object_canonical_id,\n",
        "                key=triplet.predicate.value, **edge_attrs\n",
        "            )\n",
        "            edges_added += 1\n",
        "\n",
        "    print(f\"Added {edges_added} edges (relationships) to the graph.\")\n",
        "    return graph\n",
        "\n",
        "# Let's build the graph from the state we got from our LangGraph app\n",
        "knowledge_graph = build_graph_from_state(final_ingested_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5GRtzR8u0-x"
      },
      "outputs": [],
      "source": [
        "print(f\"Graph has {knowledge_graph.number_of_nodes()} nodes and {knowledge_graph.number_of_edges()} edges.\")\n",
        "\n",
        "# Let's find the node for \"AMD\" by searching its 'name' attribute\n",
        "amd_node_id = None\n",
        "for node, data in knowledge_graph.nodes(data=True):\n",
        "    if data.get('name', '').lower() == 'amd':\n",
        "        amd_node_id = node\n",
        "        break\n",
        "\n",
        "if amd_node_id:\n",
        "    print(\"\\n--- Inspecting the 'AMD' node ---\")\n",
        "    print(f\"Attributes: {knowledge_graph.nodes[amd_node_id]}\")\n",
        "\n",
        "    print(\"\\n--- Sample Outgoing Edges from 'AMD' ---\")\n",
        "    for i, (u, v, data) in enumerate(knowledge_graph.out_edges(amd_node_id, data=True)):\n",
        "        if i >= 3: break # Show the first 3 for brevity\n",
        "        object_name = knowledge_graph.nodes[v]['name']\n",
        "        print(f\"Edge {i+1}: AMD --[{data['predicate']}]--> {object_name} (Valid From: {data['valid_at'].date()})\")\n",
        "else:\n",
        "    print(\"Could not find a node for 'AMD'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n39rmGdyvCD0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Find the 15 most connected nodes to visualize\n",
        "degrees = dict(knowledge_graph.degree())\n",
        "top_nodes = sorted(degrees, key=degrees.get, reverse=True)[:15]\n",
        "\n",
        "# Create a smaller graph containing only these top nodes\n",
        "subgraph = knowledge_graph.subgraph(top_nodes)\n",
        "\n",
        "# Draw the graph\n",
        "plt.figure(figsize=(12, 12))\n",
        "pos = nx.spring_layout(subgraph, k=0.8, iterations=50)\n",
        "labels = {node: data['name'] for node, data in subgraph.nodes(data=True)}\n",
        "nx.draw(subgraph, pos, labels=labels, with_labels=True, node_color='skyblue',\n",
        "        node_size=2500, edge_color='#666666', font_size=10)\n",
        "plt.title(\"Subgraph of Top 15 Most Connected Entities\", size=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building and Testing A Multi-Step Retrieval Agent"
      ],
      "metadata": {
        "id": "XkyzTHsd8CZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final stage moves from just storing the temporal knowledge agent to actually using it intelligently. A single step RAG can pull out only one fact at a time, but real questions often need multiple pieces of evidence stitched together. E.g. asking how AMD's focus on data centres changes from 2016 - 2017 requires pulling facts from both years, comparing them, and summarising the differences. This is where the multi-step retrieval agent comes in. it is built with three parts: a planner (which breaks the questions into steps), tools (which fetch facts from the graph), and an orchestrator (which loops between thinking and acting until the answer is ready)."
      ],
      "metadata": {
        "id": "wubkUcp37_WQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DueAXE5rvEC2"
      },
      "outputs": [],
      "source": [
        "# System prompt describes the \"persona\" for the LLM\n",
        "initial_planner_system_prompt = (\n",
        "    \"You are an expert financial research assistant. \"\n",
        "    \"Your task is to create a step-by-step plan for answering a user's question \"\n",
        "    \"by querying a temporal knowledge graph of earnings call transcripts. \"\n",
        "    \"The available tool is `factual_qa`, which can retrieve facts about an entity \"\n",
        "    \"for a specific topic (predicate) within a given date range. \"\n",
        "    \"Your plan should consist of a series of calls to this tool.\"\n",
        ")\n",
        "\n",
        "# Template for the user prompt — receives `user_question` dynamically\n",
        "initial_planner_user_prompt_template = \"\"\"\n",
        "User Question: \"{user_question}\"\n",
        "\n",
        "Based on this question, create a concise, step-by-step plan.\n",
        "Each step should be a clear action for querying the knowledge graph.\n",
        "\n",
        "Return only the plan under a heading 'Research tasks'.\n",
        "\"\"\"\n",
        "\n",
        "# Create a ChatPromptTemplate that combines the system persona and the user prompt.\n",
        "# `from_messages` takes a list of (role, content) pairs to form the conversation context.\n",
        "planner_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", initial_planner_system_prompt),          # LLM's role and behavior\n",
        "    (\"user\", initial_planner_user_prompt_template),     # Instructions for this specific run\n",
        "])\n",
        "\n",
        "# Create a \"chain\" that pipes the prompt into the LLM.\n",
        "# The `|` operator here is the LangChain \"Runnable\" syntax for composing components.\n",
        "planner_chain = planner_prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adHimXlPvHEV"
      },
      "outputs": [],
      "source": [
        "# Our sample user question for the retrieval agent\n",
        "user_question = \"How did AMD's focus on data centers evolve between 2016 and 2017?\"\n",
        "\n",
        "print(f\"--- Generating plan for question: '{user_question}' ---\")\n",
        "plan_result = planner_chain.invoke({\"user_question\": user_question})\n",
        "initial_plan = plan_result.content\n",
        "\n",
        "print(\"\\n--- Generated Plan ---\")\n",
        "print(initial_plan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YQLD3BVvHq6"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from datetime import date\n",
        "import datetime as dt # Use an alias to avoid confusion\n",
        "\n",
        "# Helper function to parse dates robustly, even if the LLM provides different formats\n",
        "def _as_datetime(ts) -> dt.datetime | None:\n",
        "    if not ts: return None\n",
        "    if isinstance(ts, dt.datetime): return ts\n",
        "    if isinstance(ts, dt.date): return dt.datetime.combine(ts, dt.datetime.min.time())\n",
        "    try:\n",
        "        return dt.datetime.strptime(ts, \"%Y-%m-%d\")\n",
        "    except (ValueError, TypeError):\n",
        "        return None\n",
        "\n",
        "@tool\n",
        "def factual_qa(entity: str, start_date: date, end_date: date, predicate: str) -> str:\n",
        "    \"\"\"\n",
        "    Queries the knowledge graph for facts about a specific entity, topic (predicate),\n",
        "    and time range. Returns a formatted string of matching relationships.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- TOOL CALL: factual_qa ---\")\n",
        "    print(f\"  - Entity: {entity}, Predicate: {predicate}, Range: {start_date} to {end_date}\")\n",
        "\n",
        "    start_dt = _as_datetime(start_date).replace(tzinfo=timezone.utc)\n",
        "    end_dt = _as_datetime(end_date).replace(tzinfo=timezone.utc)\n",
        "\n",
        "    # 1. Find the entity node in the graph using a case-insensitive search\n",
        "    target_node_id = next((nid for nid, data in knowledge_graph.nodes(data=True) if entity.lower() in data.get('name', '').lower()), None)\n",
        "    if not target_node_id: return f\"Error: Entity '{entity}' not found.\"\n",
        "\n",
        "    # 2. Search all edges connected to that node for matches\n",
        "    matching_edges = []\n",
        "    for u, v, data in knowledge_graph.edges(target_node_id, data=True):\n",
        "        if predicate.upper() in data.get('predicate', '').upper():\n",
        "            valid_at = data.get('valid_at')\n",
        "            if valid_at and start_dt <= valid_at <= end_dt:\n",
        "                subject = knowledge_graph.nodes[u]['name']\n",
        "                obj = knowledge_graph.nodes[v]['name']\n",
        "                matching_edges.append(f\"Fact: {subject} --[{data['predicate']}]--> {obj}\")\n",
        "\n",
        "    if not matching_edges: return f\"No facts found for '{entity}' with predicate '{predicate}' in that date range.\"\n",
        "    return \"\\n\".join(matching_edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfzYeV2lvPlI"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from typing import TypedDict, List\n",
        "\n",
        "# Define the state for our retrieval agent's memory\n",
        "class AgentState(TypedDict):\n",
        "    messages: List[BaseMessage]\n",
        "\n",
        "# This is the \"brain\" of our agent. It decides what to do next.\n",
        "def call_model(state: AgentState):\n",
        "    print(\"\\n--- AGENT: Calling model to decide next step... ---\")\n",
        "    response = llm_with_tools.invoke(state['messages'])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# This is a conditional edge. It checks if the LLM decided to call a tool or to finish.\n",
        "def should_continue(state: AgentState) -> str:\n",
        "    if hasattr(state['messages'][-1], 'tool_calls') and state['messages'][-1].tool_calls:\n",
        "        return \"continue_with_tool\"\n",
        "    return \"finish\"\n",
        "\n",
        "# Bind our factual_qa tool to the LLM and force it to use a tool if possible\n",
        "# This is required by our specific model\n",
        "tools = [factual_qa]\n",
        "llm_with_tools = llm.bind_tools(tools, tool_choice=\"any\")\n",
        "\n",
        "# Now, wire up the graph\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", ToolNode(tools)) # ToolNode is a pre-built node that runs our tools\n",
        "workflow.set_entry_point(\"agent\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\"continue_with_tool\": \"action\", \"finish\": END}\n",
        ")\n",
        "workflow.add_edge(\"action\", \"agent\")\n",
        "\n",
        "retrieval_agent = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss6y_v60vSt5"
      },
      "outputs": [],
      "source": [
        "# Create the initial message for the agent\n",
        "initial_message = HumanMessage(\n",
        "    content=f\"Here is my question: '{user_question}'\\n\\n\"\n",
        "            f\"Here is the plan to follow:\\n{initial_plan}\"\n",
        ")\n",
        "\n",
        "# The input to the agent is always a list of messages\n",
        "agent_input = {\"messages\": [initial_message]}\n",
        "\n",
        "print(\"--- Running the full retrieval agent ---\")\n",
        "\n",
        "# Stream the agent's execution to see its thought process in real-time\n",
        "async for output in retrieval_agent.astream(agent_input):\n",
        "    for key, value in output.items():\n",
        "        if key == \"agent\":\n",
        "            agent_message = value['messages'][-1]\n",
        "            if agent_message.tool_calls:\n",
        "                print(f\"LLM wants to call a tool: {agent_message.tool_calls[0]['name']}\")\n",
        "            else:\n",
        "                print(\"\\n--- AGENT: Final Answer ---\")\n",
        "                print(agent_message.content)\n",
        "        elif key == \"action\":\n",
        "            print(\"--- AGENT: Tool response received. ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq0_UemuvTOu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaxzeyjOdSM0iZtcbv9Xyk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}