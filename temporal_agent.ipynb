{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simranbains9810/temporal_agent/blob/main/temporal_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycM5y4eTSW9J"
      },
      "source": [
        "##Pre-processing and Analyzing our Dynamic Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFIeWIsLTKW8",
        "outputId": "6a6f5c8a-e32f-4382-c0ec-34e6a2c1b570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting datasets==2.19.0\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.19.0)\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (0.70.16)\n",
            "Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.0)\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (3.12.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.0) (6.0.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.14)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.0) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.2->datasets==2.19.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.2->datasets==2.19.0) (1.1.7)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.19.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.19.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.19.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.19.0) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.0) (2025.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.0) (1.17.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, pyarrow-hotfix, mypy-extensions, marshmallow, httpx-sse, fsspec, typing-inspect, pydantic-settings, dataclasses-json, datasets, langchain-community\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nccl-cu12==2.21.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nccl-cu12 2.23.4 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 datasets-2.19.0 fsspec-2024.3.1 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pyarrow-hotfix-0.7 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install langchain-community \"datasets==2.19.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_GZ0mscjq_d"
      },
      "source": [
        "This section walks through loading and exploring a dynamic financial dataset earnings call transcripts from Hugging Face. It uses `HuggingFaceDatasetLoader` to retrieve 188 documents, each with company metadata and date. Initial analysis shows that transcripts are evenly distributed across companies like AMD, AAPL, and NVDA, with each document averaging around 9,000 words. The code then demonstrates extracting time-related details (e.g., financial quarters) using regex to prepare for temporal analysis, setting the stage for building a knowledge base or AI agent that can handle large, time-evolving corporate data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJKj_fhTRT8h",
        "outputId": "6ff2531f-cab0-49ed-e506-50c69a2b9162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/datasets/load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import loader for Hugging Face datasets\n",
        "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n",
        "\n",
        "# Dataset configuration\n",
        "hf_dataset_name = \"jlh-ibm/earnings_call\"  # HF dataset name\n",
        "subset_name = \"transcripts\"                # Dataset subset to load\n",
        "\n",
        "# Create the loader (defaults to 'train' split)\n",
        "loader = HuggingFaceDatasetLoader(\n",
        "    path=hf_dataset_name,\n",
        "    name=subset_name,\n",
        "    page_content_column=\"transcript\"  # Column containing the main text\n",
        ")\n",
        "\n",
        "# This is the key step. The loader processes the dataset and returns a list of LangChain Document objects.\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeZmKSNKTcSh",
        "outputId": "58231ba0-d6a6-434c-9413-b2f7a936057e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 188 documents.\n"
          ]
        }
      ],
      "source": [
        "# Let's inspect the result to see the difference\n",
        "print(f\"Loaded {len(documents)} documents.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CH47ozUVYnqw",
        "outputId": "1c545b7f-f085-4738-8cf7-a72371e1d35a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total company counts:\n",
            " - AMD: 19\n",
            " - AAPL: 19\n",
            " - INTC: 19\n",
            " - MU: 17\n",
            " - GOOGL: 19\n",
            " - ASML: 19\n",
            " - CSCO: 19\n",
            " - NVDA: 19\n",
            " - AMZN: 19\n",
            " - MSFT: 19\n"
          ]
        }
      ],
      "source": [
        "# Count how many documents each company has\n",
        "company_counts = {}\n",
        "\n",
        "# Loop over all loaded documents\n",
        "for doc in documents:\n",
        "    company = doc.metadata.get(\"company\")  # Extract company from metadata\n",
        "    if company:\n",
        "        company_counts[company] = company_counts.get(company, 0) + 1\n",
        "\n",
        "# Display the counts\n",
        "print(\"Total company counts:\")\n",
        "for company, count in company_counts.items():\n",
        "    print(f\" - {company}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlLdXLKwY6wX",
        "outputId": "0be0df80-9b5a-4c7c-e9bf-9853e32253ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata for document[0]:\n",
            "{'company': 'AMD', 'date': datetime.date(2016, 7, 21)}\n",
            "\n",
            "Metadata for document[33]:\n",
            "{'company': 'AMZN', 'date': datetime.date(2019, 10, 24)}\n"
          ]
        }
      ],
      "source": [
        "# Print metadata for two sample documents (index 0 and 33)\n",
        "print(\"Metadata for document[0]:\")\n",
        "print(documents[0].metadata)\n",
        "\n",
        "print(\"\\nMetadata for document[33]:\")\n",
        "print(documents[33].metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXO6OHuyZBw9",
        "outputId": "712c6f8c-83c0-4c23-d0ef-5bf87c778a25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"\\n\\nThomson Reuters StreetEvents Event Transcript\\nE D I T E D   V E R S I O N\\n\\nQ2 2016 Advanced Micro Devices Inc Earnings Call\\nJULY 21, 2016 / 9:00PM GMT\\n\\n=====================================\n"
          ]
        }
      ],
      "source": [
        "# Print the first 200 characters of the first document's content\n",
        "first_doc = documents[0]\n",
        "print(first_doc.page_content[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0lwBkSSZCDc",
        "outputId": "8e88b3aa-836e-4f02-9ff5-04e286014259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average number of words in documents: 8797.19\n"
          ]
        }
      ],
      "source": [
        "# Calculate the average number of words per document\n",
        "total_words = sum(len(doc.page_content.split()) for doc in documents)\n",
        "average_words = total_words / len(documents) if documents else 0\n",
        "\n",
        "print(f\"Average number of words in documents: {average_words:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW_mOy_wZEAT",
        "outputId": "96433a1a-397d-406a-ef17-7c4f2a8d3b7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Quarter for the first document: Q2 2016\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# Helper function to extract a quarter string (e.g., \"Q1 2023\") from text\n",
        "def find_quarter(text: str) -> str | None:\n",
        "    \"\"\"Return the first quarter-year match found in the text, or None if absent.\"\"\"\n",
        "    # Match pattern: 'Q' followed by 1 digit, a space, and a 4-digit year\n",
        "    match = re.findall(r\"Q\\d\\s\\d{4}\", text)\n",
        "    return match[0] if match else None\n",
        "\n",
        "# Test on the first document\n",
        "quarter = find_quarter(documents[0].page_content)\n",
        "print(f\"Extracted Quarter for the first document: {quarter}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRU6ENhmk1E2"
      },
      "source": [
        "##Percentile Based Chunking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRwBQ2rylN3E"
      },
      "source": [
        "This section explains how percentile-based semantic chunking is used to split long financial transcripts into smaller, meaningful segments without losing context. Instead of cutting at fixed lengths or punctuation, each sentence is embedded with a model (Qwen3–8B via Nebius), and semantic distances between consecutive sentences are calculated. The process works as follows:\n",
        "Split text into sentences via regex.\n",
        "\n",
        "*   Split text into sentences via regex.\n",
        "*   Embed each sentence into a vector.\n",
        "*   Compute semantic distance between consecutive vectors.\n",
        "*   Determine the chosen percentile (e.g., 95th) of distances.\n",
        "*   Mark distances ≥ threshold as breakpoints.\n",
        "*   Group sentences between breakpoints into chunks, enforcing minimum size and optional overlap.\n",
        "\n",
        "Using this method, 188 long documents (≈8K words each) are transformed into 3,556 smaller chunks (≈445 words each) while retaining metadata such as company, date, and quarter for later retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nahxxALpm9Yk",
        "outputId": "9f063e23-e1bf-46b0-a02e-ef182ddaae94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-nebius in /usr/local/lib/python3.11/dist-packages (0.1.3)\n",
            "Requirement already satisfied: langchain-core>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain-nebius) (0.3.74)\n",
            "Requirement already satisfied: langchain_openai>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from langchain-nebius) (0.3.30)\n",
            "Requirement already satisfied: openai>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langchain-nebius) (1.99.9)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.35->langchain-nebius) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.35->langchain-nebius) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.35->langchain-nebius) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.35->langchain-nebius) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.35->langchain-nebius) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.35->langchain-nebius) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.35->langchain-nebius) (2.11.7)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai>=0.3.5->langchain-nebius) (0.11.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->langchain-nebius) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->langchain-nebius) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->langchain-nebius) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->langchain-nebius) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->langchain-nebius) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->langchain-nebius) (4.67.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai>=1.0.0->langchain-nebius) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0->langchain-nebius) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0->langchain-nebius) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0->langchain-nebius) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.35->langchain-nebius) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.35->langchain-nebius) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.35->langchain-nebius) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.35->langchain-nebius) (2.32.3)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.35->langchain-nebius) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core>=0.3.35->langchain-nebius) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core>=0.3.35->langchain-nebius) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core>=0.3.35->langchain-nebius) (0.4.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai>=0.3.5->langchain-nebius) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.3.35->langchain-nebius) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.3.35->langchain-nebius) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain-nebius"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ahLoXgS9pCvf"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "RKY-bRGTZIE8"
      },
      "outputs": [],
      "source": [
        "from langchain_nebius import NebiusEmbeddings\n",
        "\n",
        "# Set Nebius API key (⚠️ Avoid hardcoding secrets in production code)\n",
        "os.environ[\"NEBIUS_API_KEY\"] = \"INSERT NEBIUS KEY\"\n",
        "\n",
        "# 1. Initialize Nebius embedding model\n",
        "embeddings = NebiusEmbeddings(model=\"Qwen/Qwen3-Embedding-8B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCXdq2p3pc9n",
        "outputId": "53df6651-b801-4b8d-d420-41433607cd93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-experimental in /usr/local/lib/python3.11/dist-packages (0.3.4)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-experimental) (0.3.27)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.28 in /usr/local/lib/python3.11/dist-packages (from langchain-experimental) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.14)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (2.11.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.9)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain-experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8aTvLYXSkzq4"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "# Create a semantic chunker using percentile thresholding\n",
        "langchain_semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\",  # Use percentile-based splitting\n",
        "    breakpoint_threshold_amount=95           # split at 95th percentile\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "MUlz47C2rtfV"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkxzfsUZpIEo",
        "outputId": "ecd3bb8b-7b02-4ca0-aa9c-f3385c9ba508"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 188 documents using LangChain's SemanticChunker...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Chunking Transcripts with LangChain: 100%|██████████| 188/188 [10:09:04<00:00, 194.39s/it]\n"
          ]
        }
      ],
      "source": [
        "# Store the new, smaller chunk documents\n",
        "chunked_documents_lc = []\n",
        "\n",
        "# Printing total number of docs (188) We already know that\n",
        "print(f\"Processing {len(documents)} documents using LangChain's SemanticChunker...\")\n",
        "\n",
        "# Chunk each transcript document\n",
        "for doc in tqdm(documents, desc=\"Chunking Transcripts with LangChain\"):\n",
        "    # Extract quarter info and copy existing metadata\n",
        "    quarter = find_quarter(doc.page_content)\n",
        "    parent_metadata = doc.metadata.copy()\n",
        "    parent_metadata[\"quarter\"] = quarter\n",
        "\n",
        "    # Perform semantic chunking (returns Document objects with metadata attached)\n",
        "    chunks = langchain_semantic_chunker.create_documents(\n",
        "        [doc.page_content],\n",
        "        metadatas=[parent_metadata]\n",
        "    )\n",
        "\n",
        "    # Collect all chunks\n",
        "    chunked_documents_lc.extend(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "NCSKx3Yirkp6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d242cacc-1ae8-4570-f9d5-199aa7ed8e46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of documents (transcripts): 188\n",
            "Number of new documents (chunks): 4275\n",
            "Average chunks per transcript: 22.74\n"
          ]
        }
      ],
      "source": [
        "# Analyze the results of the LangChain chunking process\n",
        "original_doc_count = len(documents)\n",
        "chunked_doc_count = len(chunked_documents_lc)\n",
        "\n",
        "print(f\"Original number of documents (transcripts): {original_doc_count}\")\n",
        "print(f\"Number of new documents (chunks): {chunked_doc_count}\")\n",
        "print(f\"Average chunks per transcript: {chunked_doc_count / original_doc_count:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "1GRXTwGfOzPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6070ae92-cd83-47e8-c257-1855e1a9268d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Chunk Content (first 30 chars):\n",
            "No, that's a fair question, Ma...\n",
            "\n",
            "Sample Chunk Metadata:\n",
            "{'company': 'AMD', 'date': datetime.date(2016, 7, 21), 'quarter': 'Q2 2016'}\n",
            "\n",
            "Average number of words per chunk: 386.87\n"
          ]
        }
      ],
      "source": [
        "# Inspect the 11th chunk (index 10)\n",
        "sample_chunk = chunked_documents_lc[10]\n",
        "print(\"Sample Chunk Content (first 30 chars):\")\n",
        "print(sample_chunk.page_content[:30] + \"...\")\n",
        "\n",
        "print(\"\\nSample Chunk Metadata:\")\n",
        "print(sample_chunk.metadata)\n",
        "\n",
        "# Calculate average word count per chunk\n",
        "total_chunk_words = sum(len(doc.page_content.split()) for doc in chunked_documents_lc)\n",
        "average_chunk_words = total_chunk_words / chunked_doc_count if chunked_documents_lc else 0\n",
        "\n",
        "print(f\"\\nAverage number of words per chunk: {average_chunk_words:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "\n",
        "# Enum for temporal labels describing time sensitivity\n",
        "class TemporalType(str, Enum):\n",
        "    ATEMPORAL = \"ATEMPORAL\"  # Facts that are always true (e.g., \"Earth is a planet\")\n",
        "    STATIC = \"STATIC\"        # Facts about a single point in time (e.g., \"Product X launched on Jan 1st\")\n",
        "    DYNAMIC = \"DYNAMIC\"      # Facts describing an ongoing state (e.g., \"Lisa Su is the CEO\")"
      ],
      "metadata": {
        "id": "r1ElO7uAq-z7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enum for statement labels classifying statement nature\n",
        "class StatementType(str, Enum):\n",
        "    FACT = \"FACT\"            # An objective, verifiable claim\n",
        "    OPINION = \"OPINION\"      # A subjective belief or judgment\n",
        "    PREDICTION = \"PREDICTION\"  # A statement about a future event"
      ],
      "metadata": {
        "id": "pf9YVr5PsC6R"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, field_validator\n",
        "\n",
        "# This model defines the structure for a single extracted statement\n",
        "class RawStatement(BaseModel):\n",
        "    statement: str\n",
        "    statement_type: StatementType\n",
        "    temporal_type: TemporalType\n",
        "\n",
        "# This model is a container for the list of statements from one chunk\n",
        "class RawStatementList(BaseModel):\n",
        "    statements: list[RawStatement]"
      ],
      "metadata": {
        "id": "ZvF_3BXMsGqC"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These definitions provide the necessary context for the LLM to understand the labels.\n",
        "LABEL_DEFINITIONS: dict[str, dict[str, dict[str, str]]] = {\n",
        "    \"episode_labelling\": {\n",
        "        \"FACT\": dict(definition=\"Statements that are objective and can be independently verified or falsified through evidence.\"),\n",
        "        \"OPINION\": dict(definition=\"Statements that contain personal opinions, feelings, values, or judgments that are not independently verifiable.\"),\n",
        "        \"PREDICTION\": dict(definition=\"Uncertain statements about the future on something that might happen, a hypothetical outcome, unverified claims.\"),\n",
        "    },\n",
        "    \"temporal_labelling\": {\n",
        "        \"STATIC\": dict(definition=\"Often past tense, think -ed verbs, describing single points-in-time.\"),\n",
        "        \"DYNAMIC\": dict(definition=\"Often present tense, think -ing verbs, describing a period of time.\"),\n",
        "        \"ATEMPORAL\": dict(definition=\"Statements that will always hold true regardless of time.\"),\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "IPjIKE5GsIn1"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format label definitions into a clean string for prompt injection\n",
        "definitions_text = \"\"\n",
        "\n",
        "for section_key, section_dict in LABEL_DEFINITIONS.items():\n",
        "    # Add a section header with underscores replaced by spaces and uppercased\n",
        "    definitions_text += f\"==== {section_key.replace('_', ' ').upper()} DEFINITIONS ====\\n\"\n",
        "\n",
        "    # Add each category and its definition under the section\n",
        "    for category, details in section_dict.items():\n",
        "        definitions_text += f\"- {category}: {details.get('definition', '')}\\n\""
      ],
      "metadata": {
        "id": "lrq9ce0psKwD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define the prompt template for statement extraction and labeling\n",
        "statement_extraction_prompt_template = \"\"\"\n",
        "You are an expert extracting atomic statements from text.\n",
        "\n",
        "Inputs:\n",
        "- main_entity: {main_entity}\n",
        "- document_chunk: {document_chunk}\n",
        "\n",
        "Tasks:\n",
        "1. Extract clear, single-subject statements.\n",
        "2. Label each as FACT, OPINION, or PREDICTION.\n",
        "3. Label each temporally as STATIC, DYNAMIC, or ATEMPORAL.\n",
        "4. Resolve references to main_entity and include dates/quantities.\n",
        "\n",
        "Return ONLY a JSON object with the statements and labels.\n",
        "\"\"\"\n",
        "\n",
        "# Create a ChatPromptTemplate from the string template\n",
        "prompt = ChatPromptTemplate.from_template(statement_extraction_prompt_template)\n"
      ],
      "metadata": {
        "id": "izmoQNnjsNjs"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nebius import ChatNebius\n",
        "import json\n",
        "\n",
        "# Initialize our LLM\n",
        "llm = ChatNebius(model=\"deepseek-ai/DeepSeek-V3\")\n",
        "\n",
        "# Create the chain: prompt -> LLM -> structured output parser\n",
        "statement_extraction_chain = prompt | llm.with_structured_output(RawStatementList)"
      ],
      "metadata": {
        "id": "_CWSPRlSsP38"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the sample chunk we inspected earlier for testing extraction\n",
        "sample_chunk_for_extraction = chunked_documents_lc[10]\n",
        "\n",
        "print(\"--- Running statement extraction on a sample chunk ---\")\n",
        "print(f\"Chunk Content:\\n{sample_chunk_for_extraction.page_content}\")\n",
        "print(\"\\nInvoking LLM for extraction...\")\n",
        "\n",
        "# Call the extraction chain with necessary inputs\n",
        "extracted_statements_list = statement_extraction_chain.invoke({\n",
        "    \"main_entity\": sample_chunk_for_extraction.metadata[\"company\"],\n",
        "    \"publication_date\": sample_chunk_for_extraction.metadata[\"date\"].isoformat(),\n",
        "    \"document_chunk\": sample_chunk_for_extraction.page_content,\n",
        "    \"definitions\": definitions_text\n",
        "})\n",
        "\n",
        "print(\"\\n--- Extraction Result ---\")\n",
        "# Pretty-print the output JSON from the model response\n",
        "print(extracted_statements_list.model_dump_json(indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fvlKvNTsSp_",
        "outputId": "1d1e9b9f-8156-4823-e3ed-648f071cdee5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running statement extraction on a sample chunk ---\n",
            "Chunk Content:\n",
            "No, that's a fair question, Matt. So we have been very focused on the server launch for first half of 2017. Desktop should launch before that. In terms of true volume availability, I believe it will be in the first quarter of 2017. We may ship some limited volume towards the end of the fourth quarter, based on how bring-up goes and the customer readiness.\\nBut again, if I look overall at what we are trying to do, I think the desktop product is very well-positioned for that high-end desktop segment, that enthusiast segment, in both channel and OEM, which is very much a segment that AMD knows well. And so that's where we would focus -- on desktop.\\nYou should expect a notebook version of Zen with integrated graphics in 2017, and that development is going on as well. And so I think it's just a time of a lot of activity around the Zen and the different Zen product families.\\n\\n--------------------------------------------------------------------------------\\nMatt Ramsay,  Canaccord Genuity - Analyst    [22]\\n--------------------------------------------------------------------------------\\nThank you very much. Congratulations on the return to profitability.\\n\\n--------------------------------------------------------------------------------\\nLisa Su,  Advanced Micro Devices, Inc. - President and CEO    [23]\\n--------------------------------------------------------------------------------\\nThanks, Matt.\\n\\n--------------------------------------------------------------------------------\\nDevinder Kumar,  Advanced Micro Devices, Inc. - SVP, CFO and Treasurer    [24]\\n--------------------------------------------------------------------------------\\nThank you.\\n\\n--------------------------------------------------------------------------------\\nOperator    [25]\\n--------------------------------------------------------------------------------\\nStacy Rasgon, Bernstein Research.\\n\\n--------------------------------------------------------------------------------\\nStacy Rasgon,  Bernstein Research - Analyst    [26]\\n--------------------------------------------------------------------------------\\nThanks for taking my questions. I was looking at the implied guidance for Q4. You said the -- I guess for the full year, up low single digits. So, I mean, call it 3%. But it would imply Q4 down 16%/17% sequentially, and actually down on an absolute basis, lower than I would've thought. I think Q4 also has an extra week in it.\\nI was wondering if you could give us, I guess, some color on how you see the drivers, I guess, for seasonality going from Q3 to Q4 across both of the businesses, given that, I guess, the trajectory of the different product launches that we have in the back half? Like, how do you come to that number?\\n\\n--------------------------------------------------------------------------------\\nLisa Su,  Advanced Micro Devices, Inc. - President and CEO    [27]\\n--------------------------------------------------------------------------------\\nSure. Stacy, maybe I'll start and see if Devinder would like to add to it. So, look, our -- when we started the year, our expectation is that we would grow revenue in 2016 versus 2015, but we were coming off of a very low base in the first quarter. So we've been pleased with how it played out certainly in our second-quarter revenue and the third quarter revenue guidance.\\nOverall, the businesses are performing well, so we do expect both Computing and Graphics and EEFC to both grow for the year. I think the semicustom business is the large driver of the fourth quarter in terms of just how we see the overall business playing out. But the Computing and Graphics business is playing out as you might expect.\\nSo the second half should be seasonally higher, certainly with Polaris, and as we launch broader availability across the product line, as well as the seventh generation APUs as they go into back-to-school and holiday. So that's the way we should think about it.\\n\\n--------------------------------------------------------------------------------\\nStacy Rasgon,  Bernstein Research - Analyst    [28]\\n--------------------------------------------------------------------------------\\nOkay.\n",
            "\n",
            "Invoking LLM for extraction...\n",
            "\n",
            "--- Extraction Result ---\n",
            "{\n",
            "  \"statements\": [\n",
            "    {\n",
            "      \"statement\": \"AMD has been very focused on the server launch for the first half of 2017.\",\n",
            "      \"statement_type\": \"FACT\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD's desktop product should launch before the server launch in the first half of 2017.\",\n",
            "      \"statement_type\": \"PREDICTION\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD expects true volume availability for the desktop product in the first quarter of 2017.\",\n",
            "      \"statement_type\": \"PREDICTION\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD may ship some limited desktop product volume towards the end of the fourth quarter of 2016.\",\n",
            "      \"statement_type\": \"PREDICTION\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD believes the desktop product is very well-positioned for the high-end desktop segment and enthusiast segment.\",\n",
            "      \"statement_type\": \"OPINION\",\n",
            "      \"temporal_type\": \"STATIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD knows the high-end desktop and enthusiast segment well.\",\n",
            "      \"statement_type\": \"FACT\",\n",
            "      \"temporal_type\": \"STATIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD will focus on the desktop product.\",\n",
            "      \"statement_type\": \"FACT\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD expects a notebook version of Zen with integrated graphics in 2017.\",\n",
            "      \"statement_type\": \"PREDICTION\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD is developing a notebook version of Zen with integrated graphics.\",\n",
            "      \"statement_type\": \"FACT\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD is experiencing a lot of activity around the Zen and different Zen product families.\",\n",
            "      \"statement_type\": \"FACT\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD returned to profitability.\",\n",
            "      \"statement_type\": \"FACT\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD expects revenue in 2016 to grow versus 2015.\",\n",
            "      \"statement_type\": \"PREDICTION\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD's semicustom business is the large driver of the fourth quarter of 2016.\",\n",
            "      \"statement_type\": \"FACT\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD's Computing and Graphics business is performing as expected.\",\n",
            "      \"statement_type\": \"FACT\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD expects the second half of 2016 to be seasonally higher due to Polaris and broader product availability.\",\n",
            "      \"statement_type\": \"PREDICTION\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    },\n",
            "    {\n",
            "      \"statement\": \"AMD's seventh generation APUs are expected to go into back-to-school and holiday seasons in 2016.\",\n",
            "      \"statement_type\": \"PREDICTION\",\n",
            "      \"temporal_type\": \"DYNAMIC\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### OUTPUT ####\n",
        "{\n",
        "  \"statements\": [\n",
        "    {\n",
        "      \"statement\": \"AMD has been very focused on the server launch for the first half of 2017.\",\n",
        "      \"statement_type\": \"FACT\",\n",
        "      \"temporal_type\": \"DYNAMIC\"\n",
        "    },\n",
        "    {\n",
        "      \"statement\": \"AMD's Desktop product should launch before the server launch.\",\n",
        "      \"statement_type\": \"PREDICTION\",\n",
        "      \"temporal_type\": \"STATIC\"\n",
        "    },\n",
        "    {\n",
        "      \"statement\": \"AMD believes true volume availability will be in the first quarter of 2017.\",\n",
        "      \"statement_type\": \"OPINION\",\n",
        "      \"temporal_type\": \"STATIC\"\n",
        "    },\n",
        "    {\n",
        "      \"statement\": \"AMD may ship some limited volume towards the end of the fourth quarter.\",\n",
        "      \"statement_type\": \"PREDICTION\",\n",
        "      \"temporal_type\": \"STATIC\"\n",
        "    }\n",
        "  ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "lq4on-_asYpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timezone\n",
        "from dateutil.parser import parse\n",
        "import re\n",
        "\n",
        "def parse_date_str(value: str | datetime | None) -> datetime | None:\n",
        "    \"\"\"\n",
        "    Parse a string or datetime into a timezone-aware datetime object (UTC).\n",
        "    Returns None if parsing fails or input is None.\n",
        "    \"\"\"\n",
        "    if not value:\n",
        "        return None\n",
        "\n",
        "    # If already a datetime, ensure it has timezone info (UTC if missing)\n",
        "    if isinstance(value, datetime):\n",
        "        return value if value.tzinfo else value.replace(tzinfo=timezone.utc)\n",
        "\n",
        "    try:\n",
        "        # Handle year-only strings like \"2023\"\n",
        "        if re.fullmatch(r\"\\d{4}\", value.strip()):\n",
        "            year = int(value.strip())\n",
        "            return datetime(year, 1, 1, tzinfo=timezone.utc)\n",
        "\n",
        "        # Parse more complex date strings with dateutil\n",
        "        dt: datetime = parse(value)\n",
        "\n",
        "        # Ensure timezone-aware, default to UTC if missing\n",
        "        if dt.tzinfo is None:\n",
        "            dt = dt.replace(tzinfo=timezone.utc)\n",
        "\n",
        "        return dt\n",
        "    except Exception:\n",
        "        return None"
      ],
      "metadata": {
        "id": "dTgub_mGskk8"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field, field_validator\n",
        "from datetime import datetime\n",
        "\n",
        "# Model for raw temporal range with date strings as ISO 8601\n",
        "class RawTemporalRange(BaseModel):\n",
        "    valid_at: str | None = Field(None, description=\"The start date/time in ISO 8601 format.\")\n",
        "    invalid_at: str | None = Field(None, description=\"The end date/time in ISO 8601 format.\")\n",
        "\n",
        "# Model for validated temporal range with datetime objects\n",
        "class TemporalValidityRange(BaseModel):\n",
        "    valid_at: datetime | None = None\n",
        "    invalid_at: datetime | None = None\n",
        "\n",
        "    # Validator to parse date strings into datetime objects before assignment\n",
        "    @field_validator(\"valid_at\", \"invalid_at\", mode=\"before\")\n",
        "    @classmethod\n",
        "    def _parse_date_string(cls, value: str | datetime | None) -> datetime | None:\n",
        "        return parse_date_str(value)"
      ],
      "metadata": {
        "id": "5YDXNUV7slHD"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt guiding the LLM to extract temporal validity ranges from statements\n",
        "date_extraction_prompt_template = \"\"\"\n",
        "You are a temporal information extraction specialist.\n",
        "\n",
        "INPUTS:\n",
        "- statement: \"{statement}\"\n",
        "- statement_type: \"{statement_type}\"\n",
        "- temporal_type: \"{temporal_type}\"\n",
        "- publication_date: \"{publication_date}\"\n",
        "- quarter: \"{quarter}\"\n",
        "\n",
        "TASK:\n",
        "- Analyze the statement and determine the temporal validity range (valid_at, invalid_at).\n",
        "- Use the publication date as the reference point for relative expressions (e.g., \"currently\").\n",
        "- If a relationship is ongoing or its end is not specified, `invalid_at` should be null.\n",
        "\n",
        "GUIDANCE:\n",
        "- For STATIC statements, `valid_at` is the date the event occurred, and `invalid_at` is null.\n",
        "- For DYNAMIC statements, `valid_at` is when the state began, and `invalid_at` is when it ended.\n",
        "- Return dates in ISO 8601 format (e.g., YYYY-MM-DDTHH:MM:SSZ).\n",
        "\n",
        "**Output format**\n",
        "Return ONLY a valid JSON object matching the schema for `RawTemporalRange`.\n",
        "\"\"\"\n",
        "\n",
        "# Create a LangChain prompt template from the string\n",
        "date_extraction_prompt = ChatPromptTemplate.from_template(date_extraction_prompt_template)"
      ],
      "metadata": {
        "id": "fUmYsIARsnMH"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reuse the existing LLM instance.\n",
        "# Create a chain by connecting the date extraction prompt\n",
        "# with the LLM configured to output structured RawTemporalRange objects.\n",
        "date_extraction_chain = date_extraction_prompt | llm.with_structured_output(RawTemporalRange)"
      ],
      "metadata": {
        "id": "IgOFCOXisrtp"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the first extracted statement for date extraction testing\n",
        "sample_statement = extracted_statements_list.statements[0]\n",
        "chunk_metadata = sample_chunk_for_extraction.metadata\n",
        "\n",
        "print(f\"--- Running date extraction for statement ---\")\n",
        "print(f'Statement: \"{sample_statement.statement}\"')\n",
        "print(f\"Reference Publication Date: {chunk_metadata['date'].isoformat()}\")\n",
        "\n",
        "# Invoke the date extraction chain with relevant inputs\n",
        "raw_temporal_range = date_extraction_chain.invoke({\n",
        "    \"statement\": sample_statement.statement,\n",
        "    \"statement_type\": sample_statement.statement_type.value,\n",
        "    \"temporal_type\": sample_statement.temporal_type.value,\n",
        "    \"publication_date\": chunk_metadata[\"date\"].isoformat(),\n",
        "    \"quarter\": chunk_metadata[\"quarter\"]\n",
        "})\n",
        "\n",
        "# Parse and validate raw LLM output into a structured TemporalValidityRange model\n",
        "final_temporal_range = TemporalValidityRange.model_validate(raw_temporal_range.model_dump())\n",
        "\n",
        "print(\"\\n--- Parsed & Validated Result ---\")\n",
        "print(f\"Valid At: {final_temporal_range.valid_at}\")\n",
        "print(f\"Invalid At: {final_temporal_range.invalid_at}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meydLprJsu6A",
        "outputId": "b8625240-928d-4c69-cfa0-2dfcad5bf28f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running date extraction for statement ---\n",
            "Statement: \"AMD has been very focused on the server launch for the first half of 2017.\"\n",
            "Reference Publication Date: 2016-07-21\n",
            "\n",
            "--- Parsed & Validated Result ---\n",
            "Valid At: 2017-01-01 00:00:00+00:00\n",
            "Invalid At: 2017-06-30 23:59:59+00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum  # Import the Enum base class to create enumerated constants\n",
        "\n",
        "# Enum representing a fixed set of relationship predicates for graph consistency\n",
        "class Predicate(str, Enum):\n",
        "    # Each member of this Enum represents a specific type of relationship between entities\n",
        "    IS_A = \"IS_A\"                # Represents an \"is a\" relationship (e.g., a Dog IS_A Animal)\n",
        "    HAS_A = \"HAS_A\"              # Represents possession or composition (e.g., a Car HAS_A Engine)\n",
        "    LOCATED_IN = \"LOCATED_IN\"    # Represents location relationship (e.g., Store LOCATED_IN City)\n",
        "    HOLDS_ROLE = \"HOLDS_ROLE\"    # Represents role or position held (e.g., Person HOLDS_ROLE Manager)\n",
        "    PRODUCES = \"PRODUCES\"        # Represents production or creation relationship\n",
        "    SELLS = \"SELLS\"              # Represents selling relationship between entities\n",
        "    LAUNCHED = \"LAUNCHED\"        # Represents launch events (e.g., Product LAUNCHED by Company)\n",
        "    DEVELOPED = \"DEVELOPED\"      # Represents development relationship (e.g., Software DEVELOPED by Team)\n",
        "    ADOPTED_BY = \"ADOPTED_BY\"    # Represents adoption relationship (e.g., Policy ADOPTED_BY Organization)\n",
        "    INVESTS_IN = \"INVESTS_IN\"    # Represents investment relationships (e.g., Company INVESTS_IN Startup)\n",
        "    COLLABORATES_WITH = \"COLLABORATES_WITH\"  # Represents collaboration between entities\n",
        "    SUPPLIES = \"SUPPLIES\"        # Represents supplier relationship (e.g., Supplier SUPPLIES Parts)\n",
        "    HAS_REVENUE = \"HAS_REVENUE\"  # Represents revenue relationship for entities\n",
        "    INCREASED = \"INCREASED\"      # Represents an increase event or metric change\n",
        "    DECREASED = \"DECREASED\"      # Represents a decrease event or metric change\n",
        "    RESULTED_IN = \"RESULTED_IN\"  # Represents causal relationship (e.g., Event RESULTED_IN Outcome)\n",
        "    TARGETS = \"TARGETS\"          # Represents target or goal relationship\n",
        "    PART_OF = \"PART_OF\"          # Represents part-whole relationship (e.g., Wheel PART_OF Car)\n",
        "    DISCONTINUED = \"DISCONTINUED\" # Represents discontinued status or event\n",
        "    SECURED = \"SECURED\"          # Represents secured or obtained relationship (e.g., Funding SECURED by Company)"
      ],
      "metadata": {
        "id": "qcq9Fa8Nsxn7"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "\n",
        "# Model representing an entity extracted by the LLM\n",
        "class RawEntity(BaseModel):\n",
        "    entity_idx: int = Field(description=\"A temporary, 0-indexed ID for this entity.\")\n",
        "    name: str = Field(description=\"The name of the entity, e.g., 'AMD' or 'Lisa Su'.\")\n",
        "    type: str = Field(\"Unknown\", description=\"The type of entity, e.g., 'Organization', 'Person'.\")\n",
        "    description: str = Field(\"\", description=\"A brief description of the entity.\")\n",
        "\n",
        "# Model representing a single subject-predicate-object triplet\n",
        "class RawTriplet(BaseModel):\n",
        "    subject_name: str\n",
        "    subject_id: int = Field(description=\"The entity_idx of the subject.\")\n",
        "    predicate: Predicate\n",
        "    object_name: str\n",
        "    object_id: int = Field(description=\"The entity_idx of the object.\")\n",
        "    value: Optional[str] = Field(None, description=\"An optional value, e.g., '10%'.\")\n",
        "\n",
        "# Container for all entities and triplets extracted from a single statement\n",
        "class RawExtraction(BaseModel):\n",
        "    entities: List[RawEntity]\n",
        "    triplets: List[RawTriplet]"
      ],
      "metadata": {
        "id": "umy_ataXs2oC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These definitions guide the LLM in choosing the correct predicate.\n",
        "PREDICATE_DEFINITIONS = {\n",
        "    \"IS_A\": \"Denotes a class-or-type relationship (e.g., 'Model Y IS_A electric-SUV').\",\n",
        "    \"HAS_A\": \"Denotes a part-whole relationship (e.g., 'Model Y HAS_A electric-engine').\",\n",
        "    \"LOCATED_IN\": \"Specifies geographic or organisational containment.\",\n",
        "    \"HOLDS_ROLE\": \"Connects a person to a formal office or title.\",\n",
        "}\n",
        "\n",
        "# Format the predicate instructions into a string for the prompt.\n",
        "predicate_instructions_text = \"\\n\".join(f\"- {pred}: {desc}\" for pred, desc in PREDICATE_DEFINITIONS.items())"
      ],
      "metadata": {
        "id": "BilAFMTas55B"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt for extracting entities and subject-predicate-object triplets from a statement\n",
        "triplet_extraction_prompt_template = \"\"\"\n",
        "You are an information-extraction assistant.\n",
        "\n",
        "Task: From the statement, identify all entities (people, organizations, products, concepts) and all triplets (subject, predicate, object) describing their relationships.\n",
        "\n",
        "Statement: \"{statement}\"\n",
        "\n",
        "Predicate list:\n",
        "{predicate_instructions}\n",
        "\n",
        "Guidelines:\n",
        "- List entities with unique `entity_idx`.\n",
        "- List triplets linking subjects and objects by `entity_idx`.\n",
        "- Exclude temporal expressions from entities and triplets.\n",
        "\n",
        "Example:\n",
        "Statement: \"Google's revenue increased by 10% from January through March.\"\n",
        "Output: {{\n",
        "  \"entities\": [\n",
        "    {{\"entity_idx\": 0, \"name\": \"Google\", \"type\": \"Organization\", \"description\": \"A multinational technology company.\"}},\n",
        "    {{\"entity_idx\": 1, \"name\": \"Revenue\", \"type\": \"Financial Metric\", \"description\": \"Income from normal business.\"}}\n",
        "  ],\n",
        "  \"triplets\": [\n",
        "    {{\"subject_name\": \"Google\", \"subject_id\": 0, \"predicate\": \"INCREASED\", \"object_name\": \"Revenue\", \"object_id\": 1, \"value\": \"10%\"}}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Return ONLY a valid JSON object matching `RawExtraction`.\n",
        "\"\"\"\n",
        "\n",
        "# Initializing the prompt template\n",
        "triplet_extraction_prompt = ChatPromptTemplate.from_template(triplet_extraction_prompt_template)"
      ],
      "metadata": {
        "id": "1YSsX7uws8cx"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the chain for triplet and entity extraction.\n",
        "triplet_extraction_chain = triplet_extraction_prompt | llm.with_structured_output(RawExtraction)\n",
        "\n",
        "# Let's use the same statement we've been working with.\n",
        "sample_statement_for_triplets = extracted_statements_list.statements[0]\n",
        "\n",
        "print(f\"--- Running triplet extraction for statement ---\")\n",
        "print(f'Statement: \"{sample_statement_for_triplets.statement}\"')\n",
        "\n",
        "# Invoke the chain.\n",
        "raw_extraction_result = triplet_extraction_chain.invoke({\n",
        "    \"statement\": sample_statement_for_triplets.statement,\n",
        "    \"predicate_instructions\": predicate_instructions_text\n",
        "})\n",
        "\n",
        "print(\"\\n--- Triplet Extraction Result ---\")\n",
        "print(raw_extraction_result.model_dump_json(indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYR2jY96s_dv",
        "outputId": "334b4e92-10d1-47e6-b5ce-e5cd8ee5bdc0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running triplet extraction for statement ---\n",
            "Statement: \"AMD has been very focused on the server launch for the first half of 2017.\"\n",
            "\n",
            "--- Triplet Extraction Result ---\n",
            "{\n",
            "  \"entities\": [\n",
            "    {\n",
            "      \"entity_idx\": 0,\n",
            "      \"name\": \"AMD\",\n",
            "      \"type\": \"Organization\",\n",
            "      \"description\": \"A multinational semiconductor company.\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_idx\": 1,\n",
            "      \"name\": \"server launch\",\n",
            "      \"type\": \"Event\",\n",
            "      \"description\": \"The release of server-related products.\"\n",
            "    }\n",
            "  ],\n",
            "  \"triplets\": [\n",
            "    {\n",
            "      \"subject_name\": \"AMD\",\n",
            "      \"subject_id\": 0,\n",
            "      \"predicate\": \"HAS_A\",\n",
            "      \"object_name\": \"server launch\",\n",
            "      \"object_id\": 1,\n",
            "      \"value\": null\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Final persistent model for an entity in your knowledge graph\n",
        "class Entity(BaseModel):\n",
        "    id: uuid.UUID = Field(default_factory=uuid.uuid4, description=\"Unique UUID for the entity\")\n",
        "    name: str = Field(..., description=\"The name of the entity\")\n",
        "    type: str = Field(..., description=\"Entity type, e.g., 'Organization', 'Person'\")\n",
        "    description: str = Field(\"\", description=\"Brief description of the entity\")\n",
        "    resolved_id: uuid.UUID | None = Field(None, description=\"UUID of resolved entity if merged\")\n",
        "\n",
        "# Final persistent model for a triplet relationship\n",
        "class Triplet(BaseModel):\n",
        "    id: uuid.UUID = Field(default_factory=uuid.uuid4, description=\"Unique UUID for the triplet\")\n",
        "    subject_name: str = Field(..., description=\"Name of the subject entity\")\n",
        "    subject_id: uuid.UUID = Field(..., description=\"UUID of the subject entity\")\n",
        "    predicate: Predicate = Field(..., description=\"Relationship predicate\")\n",
        "    object_name: str = Field(..., description=\"Name of the object entity\")\n",
        "    object_id: uuid.UUID = Field(..., description=\"UUID of the object entity\")\n",
        "    value: str | None = Field(None, description=\"Optional value associated with the triplet\")"
      ],
      "metadata": {
        "id": "IQAQMaKntBqo"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalEvent(BaseModel):\n",
        "    \"\"\"\n",
        "    The central model that consolidates all extracted information.\n",
        "    \"\"\"\n",
        "    id: uuid.UUID = Field(default_factory=uuid.uuid4)\n",
        "    chunk_id: uuid.UUID # To link back to the original text chunk\n",
        "    statement: str\n",
        "    embedding: list[float] = [] # For similarity checks later\n",
        "\n",
        "    # Information from our previous extraction steps\n",
        "    statement_type: StatementType\n",
        "    temporal_type: TemporalType\n",
        "    valid_at: datetime | None = None\n",
        "    invalid_at: datetime | None = None\n",
        "\n",
        "    # A list of the IDs of the triplets associated with this event\n",
        "    triplets: list[uuid.UUID]\n",
        "\n",
        "    # Extra metadata for tracking changes over time\n",
        "    created_at: datetime = Field(default_factory=datetime.now)\n",
        "    expired_at: datetime | None = None\n",
        "    invalidated_by: uuid.UUID | None = None"
      ],
      "metadata": {
        "id": "WCr7MMRotIhR"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume these are already defined from previous steps:\n",
        "# sample_statement, final_temporal_range, raw_extraction_result\n",
        "\n",
        "print(\"--- Assembling the final TemporalEvent ---\")\n",
        "\n",
        "# 1. Convert raw entities to persistent Entity objects with UUIDs\n",
        "idx_to_entity_map: dict[int, Entity] = {}\n",
        "final_entities: list[Entity] = []\n",
        "\n",
        "for raw_entity in raw_extraction_result.entities:\n",
        "    entity = Entity(\n",
        "        name=raw_entity.name,\n",
        "        type=raw_entity.type,\n",
        "        description=raw_entity.description\n",
        "    )\n",
        "    idx_to_entity_map[raw_entity.entity_idx] = entity\n",
        "    final_entities.append(entity)\n",
        "\n",
        "print(f\"Created {len(final_entities)} persistent Entity objects.\")\n",
        "\n",
        "# 2. Convert raw triplets to persistent Triplet objects, linking entities via UUIDs\n",
        "final_triplets: list[Triplet] = []\n",
        "\n",
        "for raw_triplet in raw_extraction_result.triplets:\n",
        "    subject_entity = idx_to_entity_map[raw_triplet.subject_id]\n",
        "    object_entity = idx_to_entity_map[raw_triplet.object_id]\n",
        "\n",
        "    triplet = Triplet(\n",
        "        subject_name=subject_entity.name,\n",
        "        subject_id=subject_entity.id,\n",
        "        predicate=raw_triplet.predicate,\n",
        "        object_name=object_entity.name,\n",
        "        object_id=object_entity.id,\n",
        "        value=raw_triplet.value\n",
        "    )\n",
        "    final_triplets.append(triplet)\n",
        "\n",
        "print(f\"Created {len(final_triplets)} persistent Triplet objects.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKq_kj7CtKxY",
        "outputId": "8d07865b-ae45-4a5e-96c3-2135bef49aa5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Assembling the final TemporalEvent ---\n",
            "Created 2 persistent Entity objects.\n",
            "Created 1 persistent Triplet objects.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create the final TemporalEvent object\n",
        "# We'll generate a dummy chunk_id for this example.\n",
        "temporal_event = TemporalEvent(\n",
        "    chunk_id=uuid.uuid4(), # Placeholder ID\n",
        "    statement=sample_statement.statement,\n",
        "    statement_type=sample_statement.statement_type,\n",
        "    temporal_type=sample_statement.temporal_type,\n",
        "    valid_at=final_temporal_range.valid_at,\n",
        "    invalid_at=final_temporal_range.invalid_at,\n",
        "    triplets=[t.id for t in final_triplets]\n",
        ")\n",
        "\n",
        "print(\"\\n--- Final Assembled TemporalEvent ---\")\n",
        "print(temporal_event.model_dump_json(indent=2))\n",
        "\n",
        "print(\"\\n--- Associated Entities ---\")\n",
        "for entity in final_entities:\n",
        "    print(entity.model_dump_json(indent=2))\n",
        "\n",
        "print(\"\\n--- Associated Triplets ---\")\n",
        "for triplet in final_triplets:\n",
        "    print(triplet.model_dump_json(indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhXIccf_tRsC",
        "outputId": "5f789764-f98b-46d1-89c2-68c613cf523d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Assembled TemporalEvent ---\n",
            "{\n",
            "  \"id\": \"d7a2bc93-f1c2-4356-8281-4c64948b1a6b\",\n",
            "  \"chunk_id\": \"f4669226-e138-4bd4-a4e2-e6a8393fdaea\",\n",
            "  \"statement\": \"AMD has been very focused on the server launch for the first half of 2017.\",\n",
            "  \"embedding\": [],\n",
            "  \"statement_type\": \"FACT\",\n",
            "  \"temporal_type\": \"DYNAMIC\",\n",
            "  \"valid_at\": \"2017-01-01T00:00:00Z\",\n",
            "  \"invalid_at\": \"2017-06-30T23:59:59Z\",\n",
            "  \"triplets\": [\n",
            "    \"6d9294df-9693-4c47-875f-49d08083289c\"\n",
            "  ],\n",
            "  \"created_at\": \"2025-08-16T21:07:15.176944\",\n",
            "  \"expired_at\": null,\n",
            "  \"invalidated_by\": null\n",
            "}\n",
            "\n",
            "--- Associated Entities ---\n",
            "{\n",
            "  \"id\": \"ba639b08-b69e-4c9b-9189-643a5decac14\",\n",
            "  \"name\": \"AMD\",\n",
            "  \"type\": \"Organization\",\n",
            "  \"description\": \"A multinational semiconductor company.\",\n",
            "  \"resolved_id\": null\n",
            "}\n",
            "{\n",
            "  \"id\": \"b93aa6ba-3868-4bf6-857b-658049af64e2\",\n",
            "  \"name\": \"server launch\",\n",
            "  \"type\": \"Event\",\n",
            "  \"description\": \"The release of server-related products.\",\n",
            "  \"resolved_id\": null\n",
            "}\n",
            "\n",
            "--- Associated Triplets ---\n",
            "{\n",
            "  \"id\": \"6d9294df-9693-4c47-875f-49d08083289c\",\n",
            "  \"subject_name\": \"AMD\",\n",
            "  \"subject_id\": \"ba639b08-b69e-4c9b-9189-643a5decac14\",\n",
            "  \"predicate\": \"HAS_A\",\n",
            "  \"object_name\": \"server launch\",\n",
            "  \"object_id\": \"b93aa6ba-3868-4bf6-857b-658049af64e2\",\n",
            "  \"value\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, TypedDict\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    TypedDict representing the overall state of the knowledge graph ingestion.\n",
        "\n",
        "    Attributes:\n",
        "        chunks: List of Document chunks being processed.\n",
        "        temporal_events: List of TemporalEvent objects extracted from chunks.\n",
        "        entities: List of Entity objects in the graph.\n",
        "        triplets: List of Triplet objects representing relationships.\n",
        "    \"\"\"\n",
        "    chunks: List[Document]\n",
        "    temporal_events: List[TemporalEvent]\n",
        "    entities: List[Entity]\n",
        "    triplets: List[Triplet]"
      ],
      "metadata": {
        "id": "V-87-av2tUN-"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_events_from_chunks(state: GraphState) -> GraphState:\n",
        "    chunks = state[\"chunks\"]\n",
        "\n",
        "    # Extract raw statements from each chunk\n",
        "    raw_stmts = statement_extraction_chain.batch([{\n",
        "        \"main_entity\": c.metadata[\"company\"],\n",
        "        \"publication_date\": c.metadata[\"date\"].isoformat(),\n",
        "        \"document_chunk\": c.page_content,\n",
        "        \"definitions\": definitions_text\n",
        "    } for c in chunks])\n",
        "\n",
        "    # Flatten statements, attach metadata and unique chunk IDs\n",
        "    stmts = [{\"raw\": s, \"meta\": chunks[i].metadata, \"cid\": uuid.uuid4()}\n",
        "             for i, rs in enumerate(raw_stmts) for s in rs.statements]\n",
        "\n",
        "    # Prepare inputs and batch extract temporal data\n",
        "    dates = date_extraction_chain.batch([{\n",
        "        \"statement\": s[\"raw\"].statement,\n",
        "        \"statement_type\": s[\"raw\"].statement_type.value,\n",
        "        \"temporal_type\": s[\"raw\"].temporal_type.value,\n",
        "        \"publication_date\": s[\"meta\"][\"date\"].isoformat(),\n",
        "        \"quarter\": s[\"meta\"][\"quarter\"]\n",
        "    } for s in stmts])\n",
        "\n",
        "    # Prepare inputs and batch extract triplets\n",
        "    trips = triplet_extraction_chain.batch([{\n",
        "        \"statement\": s[\"raw\"].statement,\n",
        "        \"predicate_instructions\": predicate_instructions_text\n",
        "    } for s in stmts])\n",
        "\n",
        "    events, entities, triplets = [], [], []\n",
        "\n",
        "    for i, s in enumerate(stmts):\n",
        "        # Validate temporal range data\n",
        "        tr = TemporalValidityRange.model_validate(dates[i].model_dump())\n",
        "        ext = trips[i]\n",
        "\n",
        "        # Map entities by index and collect them\n",
        "        idx_map = {e.entity_idx: Entity(e.name, e.type, e.description) for e in ext.entities}\n",
        "        entities.extend(idx_map.values())\n",
        "\n",
        "        # Build triplets only if subject and object entities exist\n",
        "        tpls = [Triplet(\n",
        "            idx_map[t.subject_id].name, idx_map[t.subject_id].id, t.predicate,\n",
        "            idx_map[t.object_id].name, idx_map[t.object_id].id, t.value)\n",
        "            for t in ext.triplets if t.subject_id in idx_map and t.object_id in idx_map]\n",
        "        triplets.extend(tpls)\n",
        "\n",
        "        # Create TemporalEvent with linked triplet IDs\n",
        "        events.append(TemporalEvent(\n",
        "            chunk_id=s[\"cid\"], statement=s[\"raw\"].statement,\n",
        "            statement_type=s[\"raw\"].statement_type, temporal_type=s[\"raw\"].temporal_type,\n",
        "            valid_at=tr.valid_at, invalid_at=tr.invalid_at,\n",
        "            triplets=[t.id for t in tpls]\n",
        "        ))\n",
        "\n",
        "    return {\"chunks\": chunks, \"temporal_events\": events, \"entities\": entities, \"triplets\": triplets}"
      ],
      "metadata": {
        "id": "ELIK0SpztX9g"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3F0rU95t7El",
        "outputId": "af8eaf31-a726-4409-e0dc-43d26aa36ec8"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.6.5-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.74)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.0 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.7)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m865.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (3.11.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.32.3)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.6.5-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.2/153.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.6.5 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.0 ormsgpack-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Define a new graph using our state\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add our function as a node named \"extract_events\"\n",
        "workflow.add_node(\"extract_events\", extract_events_from_chunks)\n",
        "\n",
        "# Define the starting point of the graph\n",
        "workflow.set_entry_point(\"extract_events\")\n",
        "\n",
        "# Define the end point of the graph\n",
        "workflow.add_edge(\"extract_events\", END)\n",
        "\n",
        "# Compile the graph into a runnable application\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "jwY-4sI5tbO6"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The input is a dictionary matching our GraphState, providing the initial chunks\n",
        "graph_input = {\"chunks\": chunked_documents_lc}\n",
        "\n",
        "# Invoke the graph. This will run our entire extraction pipeline in one call.\n",
        "final_state = app.invoke(graph_input)\n"
      ],
      "metadata": {
        "id": "VRCk8jA7tfCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of objects created in the final state\n",
        "num_events = len(final_state['temporal_events'])\n",
        "num_entities = len(final_state['entities'])\n",
        "num_triplets = len(final_state['triplets'])\n",
        "\n",
        "print(f\"Total TemporalEvents created: {num_events}\")\n",
        "print(f\"Total Entities created: {num_entities}\")\n",
        "print(f\"Total Triplets created: {num_triplets}\")\n",
        "\n",
        "print(\"\\n--- Sample TemporalEvent from the final state ---\")\n",
        "# Print a sample event to see the fully assembled object\n",
        "print(final_state['temporal_events'][5].model_dump_json(indent=2))"
      ],
      "metadata": {
        "id": "cURf7gliuAKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "def setup_in_memory_db():\n",
        "    \"\"\"\n",
        "    Sets up an in-memory SQLite database and creates the 'entities' table.\n",
        "\n",
        "    The 'entities' table schema:\n",
        "    - id: TEXT, Primary Key\n",
        "    - name: TEXT, name of the entity\n",
        "    - type: TEXT, type/category of the entity\n",
        "    - description: TEXT, description of the entity\n",
        "    - is_canonical: INTEGER, flag to indicate if entity is canonical (default 1)\n",
        "\n",
        "    Returns:\n",
        "        sqlite3.Connection: A connection object to the in-memory database.\n",
        "    \"\"\"\n",
        "    # Establish connection to an in-memory SQLite database\n",
        "    conn = sqlite3.connect(\":memory:\")\n",
        "\n",
        "    # Create a cursor object to execute SQL commands\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Create the 'entities' table if it doesn't already exist\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS entities (\n",
        "            id TEXT PRIMARY KEY,\n",
        "            name TEXT,\n",
        "            type TEXT,\n",
        "            description TEXT,\n",
        "            is_canonical INTEGER DEFAULT 1\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    # Commit changes to save the table schema\n",
        "    conn.commit()\n",
        "\n",
        "    # Return the connection object for further use\n",
        "    return conn\n",
        "\n",
        "# Create the database connection and set up the entities table\n",
        "db_conn = setup_in_memory_db()"
      ],
      "metadata": {
        "id": "NW2489zbuECB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from rapidfuzz import fuzz\n",
        "from collections import defaultdict\n",
        "\n",
        "def resolve_entities_in_state(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    A LangGraph node to perform entity resolution on the extracted entities.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Entering Node: resolve_entities_in_state ---\")\n",
        "    entities = state[\"entities\"]\n",
        "    triplets = state[\"triplets\"]\n",
        "\n",
        "    cursor = db_conn.cursor()\n",
        "    cursor.execute(\"SELECT id, name FROM entities WHERE is_canonical = 1\")\n",
        "    global_canonicals = {row[1]: uuid.UUID(row[0]) for row in cursor.fetchall()}\n",
        "\n",
        "    print(f\"Starting resolution with {len(entities)} entities. Found {len(global_canonicals)} canonicals in DB.\")\n",
        "\n",
        "    # Group entities by type (e.g., 'Person', 'Organization') for more accurate matching\n",
        "    type_groups = defaultdict(list)\n",
        "    for entity in entities:\n",
        "        type_groups[entity.type].append(entity)\n",
        "\n",
        "    resolved_id_map = {} # Maps an old entity ID to its new canonical ID\n",
        "    newly_created_canonicals = {}\n",
        "\n",
        "    for entity_type, group in type_groups.items():\n",
        "        if not group: continue\n",
        "\n",
        "        # Cluster entities in the group by fuzzy name matching\n",
        "        clusters = []\n",
        "        used_indices = set()\n",
        "        for i in range(len(group)):\n",
        "            if i in used_indices: continue\n",
        "            current_cluster = [group[i]]\n",
        "            used_indices.add(i)\n",
        "            for j in range(i + 1, len(group)):\n",
        "                if j in used_indices: continue\n",
        "                # Use partial_ratio for flexible matching (e.g., \"AMD\" vs \"Advanced Micro Devices, Inc.\")\n",
        "                score = fuzz.partial_ratio(group[i].name.lower(), group[j].name.lower())\n",
        "                if score >= 80.0: # A similarity threshold of 80%\n",
        "                    current_cluster.append(group[j])\n",
        "                    used_indices.add(j)\n",
        "            clusters.append(current_cluster)\n",
        "\n",
        "        # For each cluster, find the best canonical representation (the \"medoid\")\n",
        "        for cluster in clusters:\n",
        "            scores = {e.name: sum(fuzz.ratio(e.name.lower(), other.name.lower()) for other in cluster) for e in cluster}\n",
        "            medoid_entity = max(cluster, key=lambda e: scores[e.name])\n",
        "            canonical_name = medoid_entity.name\n",
        "\n",
        "            # Check if this canonical name already exists or was just created in this run\n",
        "            if canonical_name in global_canonicals:\n",
        "                canonical_id = global_canonicals[canonical_name]\n",
        "            elif canonical_name in newly_created_canonicals:\n",
        "                canonical_id = newly_created_canonicals[canonical_name].id\n",
        "            else:\n",
        "                # Create a new canonical entity\n",
        "                canonical_id = medoid_entity.id\n",
        "                newly_created_canonicals[canonical_name] = medoid_entity\n",
        "\n",
        "            # Map all entities in this cluster to the single canonical ID\n",
        "            for entity in cluster:\n",
        "                entity.resolved_id = canonical_id\n",
        "                resolved_id_map[entity.id] = canonical_id\n",
        "\n",
        "    # Update the triplets in our state to use the new canonical IDs\n",
        "    for triplet in triplets:\n",
        "        if triplet.subject_id in resolved_id_map:\n",
        "            triplet.subject_id = resolved_id_map[triplet.subject_id]\n",
        "        if triplet.object_id in resolved_id_map:\n",
        "            triplet.object_id = resolved_id_map[triplet.object_id]\n",
        "\n",
        "    # Add any newly created canonical entities to our database\n",
        "    if newly_created_canonicals:\n",
        "        print(f\"Adding {len(newly_created_canonicals)} new canonical entities to the DB.\")\n",
        "        new_data = [(str(e.id), e.name, e.type, e.description, 1) for e in newly_created_canonicals.values()]\n",
        "        cursor.executemany(\"INSERT INTO entities (id, name, type, description, is_canonical) VALUES (?, ?, ?, ?, ?)\", new_data)\n",
        "        db_conn.commit()\n",
        "\n",
        "    print(\"Entity resolution complete.\")\n",
        "    return state"
      ],
      "metadata": {
        "id": "efCXqVDyuJL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-define the graph to include the new node\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add our two nodes to the graph\n",
        "workflow.add_node(\"extract_events\", extract_events_from_chunks)\n",
        "workflow.add_node(\"resolve_entities\", resolve_entities_in_state)\n",
        "\n",
        "# Define the new sequence of steps\n",
        "workflow.set_entry_point(\"extract_events\")\n",
        "workflow.add_edge(\"extract_events\", \"resolve_entities\")\n",
        "workflow.add_edge(\"resolve_entities\", END)\n",
        "\n",
        "# Compile the updated workflow\n",
        "app_with_resolution = workflow.compile()"
      ],
      "metadata": {
        "id": "-crazFbxuL7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the same input as before\n",
        "graph_input = {\"chunks\": chunked_documents_lc}\n",
        "\n",
        "# Invoke the new graph\n",
        "final_state_with_resolution = app_with_resolution.invoke(graph_input)"
      ],
      "metadata": {
        "id": "ULL_GVH9uN4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find a sample entity that has been resolved (i.e., has a resolved_id)\n",
        "sample_resolved_entity = next((e for e in final_state_with_resolution['entities'] if e.resolved_id is not None and e.id != e.resolved_id), None)\n",
        "\n",
        "if sample_resolved_entity:\n",
        "    print(\"\\n--- Sample of a Resolved Entity ---\")\n",
        "    print(sample_resolved_entity.model_dump_json(indent=2))\n",
        "else:\n",
        "    print(\"\\nNo sample resolved entity found (all entities were unique in this small run).\")\n",
        "\n",
        "# Check a triplet to see its updated canonical IDs\n",
        "sample_resolved_triplet = final_state_with_resolution['triplets'][0]\n",
        "print(\"\\n--- Sample Triplet with Resolved IDs ---\")\n",
        "print(sample_resolved_triplet.model_dump_json(indent=2))"
      ],
      "metadata": {
        "id": "OHkzilvcuVWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain a cursor from the existing database connection\n",
        "cursor = db_conn.cursor()\n",
        "\n",
        "# Create the 'events' table to store event-related data\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS events (\n",
        "    id TEXT PRIMARY KEY,         -- Unique identifier for each event\n",
        "    chunk_id TEXT,               -- Identifier for the chunk this event belongs to\n",
        "    statement TEXT,              -- Textual representation of the event\n",
        "    statement_type TEXT,         -- Type/category of the statement (e.g., assertion, question)\n",
        "    temporal_type TEXT,          -- Temporal classification (e.g., past, present, future)\n",
        "    valid_at TEXT,               -- Timestamp when the event becomes valid\n",
        "    invalid_at TEXT,             -- Timestamp when the event becomes invalid\n",
        "    embedding BLOB               -- Optional embedding data stored as binary (e.g., vector)\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "# Create the 'triplets' table to store relations between entities for events\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS triplets (\n",
        "    id TEXT PRIMARY KEY,         -- Unique identifier for each triplet\n",
        "    event_id TEXT,               -- Foreign key referencing 'events.id'\n",
        "    subject_id TEXT,             -- Subject entity ID in the triplet\n",
        "    predicate TEXT               -- Predicate describing relation or action\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "# Commit all changes to the in-memory database\n",
        "db_conn.commit()\n"
      ],
      "metadata": {
        "id": "gdbWZ7BOuZ3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This prompt asks the LLM to act as a referee between two events.\n",
        "event_invalidation_prompt_template = \"\"\"\n",
        "Task: Analyze the primary event against the secondary event and determine if the primary event is invalidated by the secondary event.\n",
        "Return \"True\" if the primary event is invalidated, otherwise return \"False\".\n",
        "\n",
        "Invalidation Guidelines:\n",
        "1. An event can only be invalidated if it is DYNAMIC and its `invalid_at` is currently null.\n",
        "2. A STATIC event (e.g., \"X was hired on date Y\") can invalidate a DYNAMIC event (e.g., \"Z is the current employee\").\n",
        "3. Invalidation must be a direct contradiction. For example, \"Lisa Su is CEO\" is contradicted by \"Someone else is CEO\".\n",
        "4. The invalidating event (secondary) must occur at or after the start of the primary event.\n",
        "\n",
        "---\n",
        "Primary Event (the one that might be invalidated):\n",
        "- Statement: {primary_statement}\n",
        "- Type: {primary_temporal_type}\n",
        "- Valid From: {primary_valid_at}\n",
        "- Valid To: {primary_invalid_at}\n",
        "\n",
        "Secondary Event (the new fact that might cause invalidation):\n",
        "- Statement: {secondary_statement}\n",
        "- Type: {secondary_temporal_type}\n",
        "- Valid From: {secondary_valid_at}\n",
        "---\n",
        "\n",
        "Is the primary event invalidated by the secondary event? Answer with only \"True\" or \"False\".\n",
        "\"\"\"\n",
        "\n",
        "invalidation_prompt = ChatPromptTemplate.from_template(event_invalidation_prompt_template)\n",
        "\n",
        "# This chain will output a simple string: \"True\" or \"False\".\n",
        "invalidation_chain = invalidation_prompt | llm"
      ],
      "metadata": {
        "id": "qfTi-0KWua0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def invalidate_events_in_state(state: GraphState) -> GraphState:\n",
        "    \"\"\"Mark dynamic events invalidated by later similar facts.\"\"\"\n",
        "    events = state[\"temporal_events\"]\n",
        "\n",
        "    # Embed all event statements\n",
        "    embeds = embeddings.embed_documents([e.statement for e in events])\n",
        "    for e, emb in zip(events, embeds):\n",
        "        e.embedding = emb\n",
        "\n",
        "    updates = {}\n",
        "    for i, e1 in enumerate(events):\n",
        "        # Skip non-dynamic or already invalidated events\n",
        "        if e1.temporal_type != TemporalType.DYNAMIC or e1.invalid_at:\n",
        "            continue\n",
        "\n",
        "        # Find candidate events: facts starting at or after e1 with high similarity\n",
        "        cands = [\n",
        "            e2 for j, e2 in enumerate(events) if j != i and\n",
        "            e2.statement_type == StatementType.FACT and e2.valid_at and e1.valid_at and\n",
        "            e2.valid_at >= e1.valid_at and 1 - cosine(e1.embedding, e2.embedding) > 0.5\n",
        "        ]\n",
        "        if not cands:\n",
        "            continue\n",
        "\n",
        "        # Prepare inputs for LLM invalidation check\n",
        "        inputs = [{\n",
        "            \"primary_statement\": e1.statement, \"primary_temporal_type\": e1.temporal_type.value,\n",
        "            \"primary_valid_at\": e1.valid_at.isoformat(), \"primary_invalid_at\": \"None\",\n",
        "            \"secondary_statement\": c.statement, \"secondary_temporal_type\": c.temporal_type.value,\n",
        "            \"secondary_valid_at\": c.valid_at.isoformat()\n",
        "        } for c in cands]\n",
        "\n",
        "        # Ask LLM which candidates invalidate the event\n",
        "        results = invalidation_chain.batch(inputs)\n",
        "\n",
        "        # Record earliest invalidation info\n",
        "        for c, r in zip(cands, results):\n",
        "            if r.content.strip().lower() == \"true\" and (e1.id not in updates or c.valid_at < updates[e1.id][\"invalid_at\"]):\n",
        "                updates[e1.id] = {\"invalid_at\": c.valid_at, \"invalidated_by\": c.id}\n",
        "\n",
        "    # Apply invalidations to events\n",
        "    for e in events:\n",
        "        if e.id in updates:\n",
        "            e.invalid_at = updates[e.id][\"invalid_at\"]\n",
        "            e.invalidated_by = updates[e.id][\"invalidated_by\"]\n",
        "\n",
        "    return state"
      ],
      "metadata": {
        "id": "QGoUjTHKuftF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-define the graph to include all three nodes\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "workflow.add_node(\"extract_events\", extract_events_from_chunks)\n",
        "workflow.add_node(\"resolve_entities\", resolve_entities_in_state)\n",
        "workflow.add_node(\"invalidate_events\", invalidate_events_in_state)\n",
        "\n",
        "# Define the complete pipeline flow\n",
        "workflow.set_entry_point(\"extract_events\")\n",
        "workflow.add_edge(\"extract_events\", \"resolve_entities\")\n",
        "workflow.add_edge(\"resolve_entities\", \"invalidate_events\")\n",
        "workflow.add_edge(\"invalidate_events\", END)\n",
        "\n",
        "# Compile the final ingestion workflow\n",
        "ingestion_app = workflow.compile()"
      ],
      "metadata": {
        "id": "hy5Ccjxmuh1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the same input as before\n",
        "graph_input = {\"chunks\": chunked_documents_lc}\n",
        "\n",
        "# Invoke the final graph\n",
        "final_ingested_state = ingestion_app.invoke(graph_input)\n",
        "print(\"\\n--- Full graph execution with invalidation complete ---\")"
      ],
      "metadata": {
        "id": "6XsrDX9cuifd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find and print an invalidated event from the final state\n",
        "invalidated_event = next((e for e in final_ingested_state['temporal_events'] if e.invalidated_by is not None), None)\n",
        "\n",
        "if invalidated_event:\n",
        "    print(\"\\n--- Sample of an Invalidated Event ---\")\n",
        "    print(invalidated_event.model_dump_json(indent=2))\n",
        "\n",
        "    # Find the event that caused the invalidation\n",
        "    invalidating_event = next((e for e in final_ingested_state['temporal_events'] if e.id == invalidated_event.invalidated_by), None)\n",
        "\n",
        "    if invalidating_event:\n",
        "        print(\"\\n--- Was Invalidated By this Event ---\")\n",
        "        print(invalidating_event.model_dump_json(indent=2))\n",
        "else:\n",
        "    print(\"\\nNo invalidated events were found in this run.\")"
      ],
      "metadata": {
        "id": "OShOPeSNutBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import uuid\n",
        "\n",
        "def build_graph_from_state(state: GraphState) -> nx.MultiDiGraph:\n",
        "    \"\"\"\n",
        "    Builds a NetworkX graph from the final state of our ingestion pipeline.\n",
        "    \"\"\"\n",
        "    print(\"--- Building Knowledge Graph from final state ---\")\n",
        "\n",
        "    entities = state[\"entities\"]\n",
        "    triplets = state[\"triplets\"]\n",
        "    temporal_events = state[\"temporal_events\"]\n",
        "\n",
        "    # Create a quick-lookup map from an entity's ID to the entity object itself\n",
        "    entity_map = {entity.id: entity for entity in entities}\n",
        "\n",
        "    graph = nx.MultiDiGraph() # A directed graph that allows multiple edges\n",
        "\n",
        "    # 1. Add a node for each unique, canonical entity\n",
        "    canonical_ids = {e.resolved_id if e.resolved_id else e.id for e in entities}\n",
        "    for canonical_id in canonical_ids:\n",
        "        # Find the entity object that represents this canonical ID\n",
        "        canonical_entity_obj = entity_map.get(canonical_id)\n",
        "        if canonical_entity_obj:\n",
        "            graph.add_node(\n",
        "                str(canonical_id), # Node names in NetworkX are typically strings\n",
        "                name=canonical_entity_obj.name,\n",
        "                type=canonical_entity_obj.type,\n",
        "                description=canonical_entity_obj.description\n",
        "            )\n",
        "\n",
        "    print(f\"Added {graph.number_of_nodes()} canonical entity nodes to the graph.\")\n",
        "\n",
        "    # 2. Add an edge for each triplet, decorated with temporal info\n",
        "    edges_added = 0\n",
        "    event_map = {event.id: event for event in temporal_events}\n",
        "    for triplet in triplets:\n",
        "        # Find the parent event that this triplet belongs to\n",
        "        parent_event = next((ev for ev in temporal_events if triplet.id in ev.triplets), None)\n",
        "        if not parent_event: continue\n",
        "\n",
        "        # Get the canonical IDs for the subject and object\n",
        "        subject_canonical_id = str(triplet.subject_id)\n",
        "        object_canonical_id = str(triplet.object_id)\n",
        "\n",
        "        # Add the edge to the graph\n",
        "        if graph.has_node(subject_canonical_id) and graph.has_node(object_canonical_id):\n",
        "            edge_attrs = {\n",
        "                \"predicate\": triplet.predicate.value, \"value\": triplet.value,\n",
        "                \"statement\": parent_event.statement, \"valid_at\": parent_event.valid_at,\n",
        "                \"invalid_at\": parent_event.invalid_at,\n",
        "                \"statement_type\": parent_event.statement_type.value\n",
        "            }\n",
        "            graph.add_edge(\n",
        "                subject_canonical_id, object_canonical_id,\n",
        "                key=triplet.predicate.value, **edge_attrs\n",
        "            )\n",
        "            edges_added += 1\n",
        "\n",
        "    print(f\"Added {edges_added} edges (relationships) to the graph.\")\n",
        "    return graph\n",
        "\n",
        "# Let's build the graph from the state we got from our LangGraph app\n",
        "knowledge_graph = build_graph_from_state(final_ingested_state)"
      ],
      "metadata": {
        "id": "oE_i_n9pu0WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Graph has {knowledge_graph.number_of_nodes()} nodes and {knowledge_graph.number_of_edges()} edges.\")\n",
        "\n",
        "# Let's find the node for \"AMD\" by searching its 'name' attribute\n",
        "amd_node_id = None\n",
        "for node, data in knowledge_graph.nodes(data=True):\n",
        "    if data.get('name', '').lower() == 'amd':\n",
        "        amd_node_id = node\n",
        "        break\n",
        "\n",
        "if amd_node_id:\n",
        "    print(\"\\n--- Inspecting the 'AMD' node ---\")\n",
        "    print(f\"Attributes: {knowledge_graph.nodes[amd_node_id]}\")\n",
        "\n",
        "    print(\"\\n--- Sample Outgoing Edges from 'AMD' ---\")\n",
        "    for i, (u, v, data) in enumerate(knowledge_graph.out_edges(amd_node_id, data=True)):\n",
        "        if i >= 3: break # Show the first 3 for brevity\n",
        "        object_name = knowledge_graph.nodes[v]['name']\n",
        "        print(f\"Edge {i+1}: AMD --[{data['predicate']}]--> {object_name} (Valid From: {data['valid_at'].date()})\")\n",
        "else:\n",
        "    print(\"Could not find a node for 'AMD'.\")"
      ],
      "metadata": {
        "id": "h5GRtzR8u0-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Find the 15 most connected nodes to visualize\n",
        "degrees = dict(knowledge_graph.degree())\n",
        "top_nodes = sorted(degrees, key=degrees.get, reverse=True)[:15]\n",
        "\n",
        "# Create a smaller graph containing only these top nodes\n",
        "subgraph = knowledge_graph.subgraph(top_nodes)\n",
        "\n",
        "# Draw the graph\n",
        "plt.figure(figsize=(12, 12))\n",
        "pos = nx.spring_layout(subgraph, k=0.8, iterations=50)\n",
        "labels = {node: data['name'] for node, data in subgraph.nodes(data=True)}\n",
        "nx.draw(subgraph, pos, labels=labels, with_labels=True, node_color='skyblue',\n",
        "        node_size=2500, edge_color='#666666', font_size=10)\n",
        "plt.title(\"Subgraph of Top 15 Most Connected Entities\", size=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n39rmGdyvCD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# System prompt describes the \"persona\" for the LLM\n",
        "initial_planner_system_prompt = (\n",
        "    \"You are an expert financial research assistant. \"\n",
        "    \"Your task is to create a step-by-step plan for answering a user's question \"\n",
        "    \"by querying a temporal knowledge graph of earnings call transcripts. \"\n",
        "    \"The available tool is `factual_qa`, which can retrieve facts about an entity \"\n",
        "    \"for a specific topic (predicate) within a given date range. \"\n",
        "    \"Your plan should consist of a series of calls to this tool.\"\n",
        ")\n",
        "\n",
        "# Template for the user prompt — receives `user_question` dynamically\n",
        "initial_planner_user_prompt_template = \"\"\"\n",
        "User Question: \"{user_question}\"\n",
        "\n",
        "Based on this question, create a concise, step-by-step plan.\n",
        "Each step should be a clear action for querying the knowledge graph.\n",
        "\n",
        "Return only the plan under a heading 'Research tasks'.\n",
        "\"\"\"\n",
        "\n",
        "# Create a ChatPromptTemplate that combines the system persona and the user prompt.\n",
        "# `from_messages` takes a list of (role, content) pairs to form the conversation context.\n",
        "planner_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", initial_planner_system_prompt),          # LLM's role and behavior\n",
        "    (\"user\", initial_planner_user_prompt_template),     # Instructions for this specific run\n",
        "])\n",
        "\n",
        "# Create a \"chain\" that pipes the prompt into the LLM.\n",
        "# The `|` operator here is the LangChain \"Runnable\" syntax for composing components.\n",
        "planner_chain = planner_prompt | llm"
      ],
      "metadata": {
        "id": "DueAXE5rvEC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our sample user question for the retrieval agent\n",
        "user_question = \"How did AMD's focus on data centers evolve between 2016 and 2017?\"\n",
        "\n",
        "print(f\"--- Generating plan for question: '{user_question}' ---\")\n",
        "plan_result = planner_chain.invoke({\"user_question\": user_question})\n",
        "initial_plan = plan_result.content\n",
        "\n",
        "print(\"\\n--- Generated Plan ---\")\n",
        "print(initial_plan)"
      ],
      "metadata": {
        "id": "adHimXlPvHEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "from datetime import date\n",
        "import datetime as dt # Use an alias to avoid confusion\n",
        "\n",
        "# Helper function to parse dates robustly, even if the LLM provides different formats\n",
        "def _as_datetime(ts) -> dt.datetime | None:\n",
        "    if not ts: return None\n",
        "    if isinstance(ts, dt.datetime): return ts\n",
        "    if isinstance(ts, dt.date): return dt.datetime.combine(ts, dt.datetime.min.time())\n",
        "    try:\n",
        "        return dt.datetime.strptime(ts, \"%Y-%m-%d\")\n",
        "    except (ValueError, TypeError):\n",
        "        return None\n",
        "\n",
        "@tool\n",
        "def factual_qa(entity: str, start_date: date, end_date: date, predicate: str) -> str:\n",
        "    \"\"\"\n",
        "    Queries the knowledge graph for facts about a specific entity, topic (predicate),\n",
        "    and time range. Returns a formatted string of matching relationships.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- TOOL CALL: factual_qa ---\")\n",
        "    print(f\"  - Entity: {entity}, Predicate: {predicate}, Range: {start_date} to {end_date}\")\n",
        "\n",
        "    start_dt = _as_datetime(start_date).replace(tzinfo=timezone.utc)\n",
        "    end_dt = _as_datetime(end_date).replace(tzinfo=timezone.utc)\n",
        "\n",
        "    # 1. Find the entity node in the graph using a case-insensitive search\n",
        "    target_node_id = next((nid for nid, data in knowledge_graph.nodes(data=True) if entity.lower() in data.get('name', '').lower()), None)\n",
        "    if not target_node_id: return f\"Error: Entity '{entity}' not found.\"\n",
        "\n",
        "    # 2. Search all edges connected to that node for matches\n",
        "    matching_edges = []\n",
        "    for u, v, data in knowledge_graph.edges(target_node_id, data=True):\n",
        "        if predicate.upper() in data.get('predicate', '').upper():\n",
        "            valid_at = data.get('valid_at')\n",
        "            if valid_at and start_dt <= valid_at <= end_dt:\n",
        "                subject = knowledge_graph.nodes[u]['name']\n",
        "                obj = knowledge_graph.nodes[v]['name']\n",
        "                matching_edges.append(f\"Fact: {subject} --[{data['predicate']}]--> {obj}\")\n",
        "\n",
        "    if not matching_edges: return f\"No facts found for '{entity}' with predicate '{predicate}' in that date range.\"\n",
        "    return \"\\n\".join(matching_edges)"
      ],
      "metadata": {
        "id": "5YQLD3BVvHq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from typing import TypedDict, List\n",
        "\n",
        "# Define the state for our retrieval agent's memory\n",
        "class AgentState(TypedDict):\n",
        "    messages: List[BaseMessage]\n",
        "\n",
        "# This is the \"brain\" of our agent. It decides what to do next.\n",
        "def call_model(state: AgentState):\n",
        "    print(\"\\n--- AGENT: Calling model to decide next step... ---\")\n",
        "    response = llm_with_tools.invoke(state['messages'])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# This is a conditional edge. It checks if the LLM decided to call a tool or to finish.\n",
        "def should_continue(state: AgentState) -> str:\n",
        "    if hasattr(state['messages'][-1], 'tool_calls') and state['messages'][-1].tool_calls:\n",
        "        return \"continue_with_tool\"\n",
        "    return \"finish\"\n",
        "\n",
        "# Bind our factual_qa tool to the LLM and force it to use a tool if possible\n",
        "# This is required by our specific model\n",
        "tools = [factual_qa]\n",
        "llm_with_tools = llm.bind_tools(tools, tool_choice=\"any\")\n",
        "\n",
        "# Now, wire up the graph\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", ToolNode(tools)) # ToolNode is a pre-built node that runs our tools\n",
        "workflow.set_entry_point(\"agent\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\"continue_with_tool\": \"action\", \"finish\": END}\n",
        ")\n",
        "workflow.add_edge(\"action\", \"agent\")\n",
        "\n",
        "retrieval_agent = workflow.compile()"
      ],
      "metadata": {
        "id": "wfzYeV2lvPlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the initial message for the agent\n",
        "initial_message = HumanMessage(\n",
        "    content=f\"Here is my question: '{user_question}'\\n\\n\"\n",
        "            f\"Here is the plan to follow:\\n{initial_plan}\"\n",
        ")\n",
        "\n",
        "# The input to the agent is always a list of messages\n",
        "agent_input = {\"messages\": [initial_message]}\n",
        "\n",
        "print(\"--- Running the full retrieval agent ---\")\n",
        "\n",
        "# Stream the agent's execution to see its thought process in real-time\n",
        "async for output in retrieval_agent.astream(agent_input):\n",
        "    for key, value in output.items():\n",
        "        if key == \"agent\":\n",
        "            agent_message = value['messages'][-1]\n",
        "            if agent_message.tool_calls:\n",
        "                print(f\"LLM wants to call a tool: {agent_message.tool_calls[0]['name']}\")\n",
        "            else:\n",
        "                print(\"\\n--- AGENT: Final Answer ---\")\n",
        "                print(agent_message.content)\n",
        "        elif key == \"action\":\n",
        "            print(\"--- AGENT: Tool response received. ---\")"
      ],
      "metadata": {
        "id": "ss6y_v60vSt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bq0_UemuvTOu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIy3jHbCSnJNo3H9z2r0Cu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}